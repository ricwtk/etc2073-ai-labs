{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ETC2073 Artificial Intelligence This site hosts the lab sheets for the module of ETC2073 Artificial Intelligence in the Department of Engineering (DEN) in Sunway University. Aim The aim of these labs is to guide the students to implement the basic artificial intelligence (AI) algorithms with and/or without Python libraries. Information The labs are designed to follow the schedule of the lectures, therefore you will require the knowledge of the previous lectures to be able to conduct the lab. Schedule The schedule is subject to change. Week 1 Getting started Week 2-3 Lab 1 Introductory Python Week 4 Lab 2 Fuzzy Systems Week 5 Lab 3 Genetic Algorithm Week 6 Lab 4 Particle Swarm Optimisation Week 7 Lab 5 Machine Learning","title":"Overview"},{"location":"#aim","text":"The aim of these labs is to guide the students to implement the basic artificial intelligence (AI) algorithms with and/or without Python libraries.","title":"Aim"},{"location":"#information","text":"The labs are designed to follow the schedule of the lectures, therefore you will require the knowledge of the previous lectures to be able to conduct the lab.","title":"Information"},{"location":"#schedule","text":"The schedule is subject to change. Week 1 Getting started Week 2-3 Lab 1 Introductory Python Week 4 Lab 2 Fuzzy Systems Week 5 Lab 3 Genetic Algorithm Week 6 Lab 4 Particle Swarm Optimisation Week 7 Lab 5 Machine Learning","title":"Schedule"},{"location":"get-start/","text":"Getting started The labs for ETC2073 Artificial Intelligence will be using Python as the programming language. The Anaconda distribution of Python is recommended, however, if you are familiar and comfortable with the vanilla distribution of Python. For those who have not installed Python in their machine, proceed to next session . This page only covers the installation of the Anaconda platform as, in my opinion, it is more beginner friendly in terms of installing packages and encapsulating environments. Installation Download the Anaconda installer with Python 3 for your system from https://www.anaconda.com/download . Use the graphical installer to install Anaconda. Launching IDE You will be using the Spyder IDE for Python. Feel free to use other IDE or code editor with terminal if that's your preference. Start the Anaconda Navigator from your application list. From Anaconda base environment, launch Spyder IDE. The Spyder IDE consists of three parts: the editor, the variable explorer, and the IPython Console. The editor is where you write your codes. The variable explorer shows the value of the variable after running the code. The IPython console allows you to execute commands, interact with the running code, and display visualisation. After the code is written in the editor, you can execute the code using F5 to run file. Then the variables and their values can be found in the variable explorer.","title":"Getting started"},{"location":"get-start/#getting-started","text":"The labs for ETC2073 Artificial Intelligence will be using Python as the programming language. The Anaconda distribution of Python is recommended, however, if you are familiar and comfortable with the vanilla distribution of Python. For those who have not installed Python in their machine, proceed to next session . This page only covers the installation of the Anaconda platform as, in my opinion, it is more beginner friendly in terms of installing packages and encapsulating environments.","title":"Getting started"},{"location":"get-start/#installation","text":"Download the Anaconda installer with Python 3 for your system from https://www.anaconda.com/download . Use the graphical installer to install Anaconda.","title":"Installation"},{"location":"get-start/#launching-ide","text":"You will be using the Spyder IDE for Python. Feel free to use other IDE or code editor with terminal if that's your preference. Start the Anaconda Navigator from your application list. From Anaconda base environment, launch Spyder IDE. The Spyder IDE consists of three parts: the editor, the variable explorer, and the IPython Console. The editor is where you write your codes. The variable explorer shows the value of the variable after running the code. The IPython console allows you to execute commands, interact with the running code, and display visualisation. After the code is written in the editor, you can execute the code using F5 to run file. Then the variables and their values can be found in the variable explorer.","title":"Launching IDE"},{"location":"lab1-introductory-python/","text":"Lab 1: Introduction to Python Objective To understand basic syntax of Python programming language. Declare a variable Python is strongly and dynamically typed. Strongly typed means the type of a variable does not change unexpectedly. When a variable is defined as a string with only numerical digits, it stays string, it doesn\u2019t become an integer or number. Dynamically typed means the type of a variable can change when a value of a different type is assigned to the variable. As Python is dynamically typed, we do not need to specify a type when we declare a variable. We just assign a value to the variable. Define a variable named val and assign the value 'a' to the variable. val = 'a' The type of the variable can be viewed in the variable explorer. Continue from the previous code block, assign the value 1 to the same variable. The variable will now be of type int instead of type str . val = 1 Array manipulation Array in Python can be declared using a set of square brackets ( [...] ). To assign a variable with an empty array, arr = [] To define an array with a series of numbers, arr = [ 1 , 2 , 3 , 4 , 5 ] To define an array with a series of alphabets, arr = [ 'a' , 'b' , 'c' , 'd' , 'e' ] An array can also be defined with values of mixed types. arr = [ 1 , 'a' , 2 , 'b' , 3 , 'c' ] Python arrays (or more commonly known as lists) are zero indexed arrays; it means to access the first element in the array arr , # in IPython console arr [ 0 ] # gives the output of 1 arr [ 1 ] # gives the output of 'a' Python arrays also support negative indexing; this means to get the last element in the array arr , # in IPython console arr [ - 1 ] # gives the output of 'c' Colon ( : ) can be used to extract multiple elements from an array. Maximum two (2) colons can be used for indexing/slicing an array. arr[0:5:2] The value before the first colon is the starting index, the value after the first colon is the ending index (exclusive), the value after the second colon is the number of steps. If the first value is empty, it is assumed as 0 . If the second value is empty, it is assumed as the length of the array, i.e. up till the last element in the array. If the third value is empty, it is assumed as 1 . # in IPython console arr # [1, 'a', 2, 'b', 3, 'c'] arr [ 1 : 5 ] # ['a', 2, 'b', 3] arr [ 1 : 5 : 2 ] # ['a', 'b'] arr [: 3 ] # [1, 'a', 2] arr [ 4 :] # [3, 'c'] arr [ 2 : - 2 ] # [2, 'b'] arr [ 4 : 1 ] # [] arr [ 4 : 1 : - 1 ] # [3, 'b', 2] --> slice in the reverse order Add an element to the end of an array # in IPython console arr . append ( 4 ) arr # [1, 'a', 2, 'b', 3, 'c', 4] Add multiple elements to the end of an array # in IPython console arr . extend ([ 'd' , 5 , 'e' ]) arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Assign a value to a specific index # in IPython console arr [ 0 ] = 0 arr # [0, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Insert an element at a specific index # in IPython console arr . insert ( 1 , 1 ) arr # [0, 1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Remove an element at a specific index # in IPython console del arr [ 0 ] arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Combining arrays zip can be used to combine two or more arrays. # in editor joint_arr = list ( zip ([ 1 , 2 , 3 ], [ 'a' , 'b' , 'c' ])) # in IPython console joint_arr # [(1,'a'),(2,'b'),(3,'c')] Loops Python supports for and while loops. To loop through every element in the array arr and print them to the console, for a in arr : print ( a ) # 1 # a # 2 # ... for a in arr : loops through all elements in arr and in each loop, an element in arr is assigned to the variable a . Note that in most other programming languages, code blocks are separated with delimiters such as the curly brackets ( {} ). This is not the case in Python. Code blocks in Python are defined by their indentation and normally initiated with a colon( : ). For example, for a in arr : print ( a ) print ( arr ) print(a) is the command to be executed in each loop. print(arr) is only executed after the for loop is completed. for a in arr : print ( a ) print ( arr ) In this case, print(arr) is executed in each loop. Using zip we can loop through two arrays at once. for item in zip ([ 'a' , 'b' , 'c' , 'd' ],[ 'artificial' , 'breadth' , 'cost' , 'depth' ]): print ( item [ 0 ] + ' for ' + item [ 1 ]) enumerate is useful in obtaining the index of the element in the array. for ( index , item ) in enumerate ([ 'a' , 'b' , 'c' , 'd' ]): print ( 'Index of ' + item + ' is: ' + str ( index )) Looping through a dictionary ( dict ) can also be done easily. x_dict = { 'd' : 'depth' , 'e' : 'estimation' , 'f' : 'frontier' } for key , value in x_dict . items (): print ( key + ' for ' + value ) while loops can be defined similarly. x = 0 while x != 10 : x += 1 print ( x ) print ( 'while loop is completed' ) The following example introduces nested array and multiple assignments. arr = [[ 'a' , 1 ],[ 'b' , 2 ],[ 'c' , 3 ],[ 'd' , 4 ],[ 'e' , 5 ],[ 'f' , 6 ]] for [ a , n ] in arr : print ( str ( a ) + ' is ' + str ( n )) Conditions The following operators can be used for conditional testing: Operator Definition == Equivalence != Inequivalence < Less than <= Less than or equal to > Greater than >= Greater than or equal to Python also supports text operator for conditional testing: Operator Definition Example (symbolic) Example (text) is Equivalence a == 1 a is 1 not Inequivalence a != 1 a is not 1 or not (a is 1) or not a is 1 or not(a == 1) Combining two conditions can also be done with text operators and and or . Symbolic operator Text operator & and | or User-defined functions To define a custom function with the name of custom_fcn , def custom_fcn (): print ( 'This is a custom function to display custom message' ) To call the function, custom_fcn () # This is a custom function to display custom message User-defined functions with input arguments To define a custom function with input arguments, def custom_fcn ( msg ): print ( 'This is a custom function to display ' + msg ) The function can be called by custom_fcn ( 'new message' ) # This is a custom function to display new message User-defined functions with optional input arguments To define a custom function with optional input arguments, we just need to provide the default value to the optional input arguments. def custom_fcn ( msg = 'default message' ): print ( 'This is a custom function to display ' + msg ) The input arguments can also be specified as named inputs. custom_fcn ( msg = 'new message' ) # This is a custom function to display new message Exercise Create a new file in Spyder. Define a variable named friends such that it is a nested array in which contains the name, home country, and home state/province of 10 of your friends (real or virtual). For example, friends = [[ \"James\" , \"Malaysia\" , \"Malacca\" ], [ \"Goh\" , \"Australia\" , \"Brisbane\" ], [ \"Don\" , \"Malaysia\" , \"Pahang\" ]] Create a function in the same file with three (3) optional input arguments, name , home_country , home_state . def filterFriend ( name = \"\" , home_country = \"\" , home_state = \"\" ): ... return filtered This function will filter friends based on the input arguments provided. The function will ignore the input argument if it has empty string, i.e. \"\" . If any of the input arguments is provided, the function will find the friends whose detail(s) matches the input. For example, filterFriend(name=\"James\") will return [[\"James\", \"Malaysia\", \"Malacca\"]] filterFriend(home_country=\"Malaysia\") will return [[\"James\", \"Malaysia\", \"Malacca\"], [\"Don\", \"Malaysia\", \"Pahang\"]] .","title":"Lab 1: Introduction to Python"},{"location":"lab1-introductory-python/#lab-1-introduction-to-python","text":"","title":"Lab 1: Introduction to Python"},{"location":"lab1-introductory-python/#objective","text":"To understand basic syntax of Python programming language.","title":"Objective"},{"location":"lab1-introductory-python/#declare-a-variable","text":"Python is strongly and dynamically typed. Strongly typed means the type of a variable does not change unexpectedly. When a variable is defined as a string with only numerical digits, it stays string, it doesn\u2019t become an integer or number. Dynamically typed means the type of a variable can change when a value of a different type is assigned to the variable. As Python is dynamically typed, we do not need to specify a type when we declare a variable. We just assign a value to the variable. Define a variable named val and assign the value 'a' to the variable. val = 'a' The type of the variable can be viewed in the variable explorer. Continue from the previous code block, assign the value 1 to the same variable. The variable will now be of type int instead of type str . val = 1","title":"Declare a variable"},{"location":"lab1-introductory-python/#array-manipulation","text":"Array in Python can be declared using a set of square brackets ( [...] ). To assign a variable with an empty array, arr = [] To define an array with a series of numbers, arr = [ 1 , 2 , 3 , 4 , 5 ] To define an array with a series of alphabets, arr = [ 'a' , 'b' , 'c' , 'd' , 'e' ] An array can also be defined with values of mixed types. arr = [ 1 , 'a' , 2 , 'b' , 3 , 'c' ] Python arrays (or more commonly known as lists) are zero indexed arrays; it means to access the first element in the array arr , # in IPython console arr [ 0 ] # gives the output of 1 arr [ 1 ] # gives the output of 'a' Python arrays also support negative indexing; this means to get the last element in the array arr , # in IPython console arr [ - 1 ] # gives the output of 'c' Colon ( : ) can be used to extract multiple elements from an array. Maximum two (2) colons can be used for indexing/slicing an array. arr[0:5:2] The value before the first colon is the starting index, the value after the first colon is the ending index (exclusive), the value after the second colon is the number of steps. If the first value is empty, it is assumed as 0 . If the second value is empty, it is assumed as the length of the array, i.e. up till the last element in the array. If the third value is empty, it is assumed as 1 . # in IPython console arr # [1, 'a', 2, 'b', 3, 'c'] arr [ 1 : 5 ] # ['a', 2, 'b', 3] arr [ 1 : 5 : 2 ] # ['a', 'b'] arr [: 3 ] # [1, 'a', 2] arr [ 4 :] # [3, 'c'] arr [ 2 : - 2 ] # [2, 'b'] arr [ 4 : 1 ] # [] arr [ 4 : 1 : - 1 ] # [3, 'b', 2] --> slice in the reverse order","title":"Array manipulation"},{"location":"lab1-introductory-python/#add-an-element-to-the-end-of-an-array","text":"# in IPython console arr . append ( 4 ) arr # [1, 'a', 2, 'b', 3, 'c', 4]","title":"Add an element to the end of an array"},{"location":"lab1-introductory-python/#add-multiple-elements-to-the-end-of-an-array","text":"# in IPython console arr . extend ([ 'd' , 5 , 'e' ]) arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Add multiple elements to the end of an array"},{"location":"lab1-introductory-python/#assign-a-value-to-a-specific-index","text":"# in IPython console arr [ 0 ] = 0 arr # [0, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Assign a value to a specific index"},{"location":"lab1-introductory-python/#insert-an-element-at-a-specific-index","text":"# in IPython console arr . insert ( 1 , 1 ) arr # [0, 1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Insert an element at a specific index"},{"location":"lab1-introductory-python/#remove-an-element-at-a-specific-index","text":"# in IPython console del arr [ 0 ] arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Remove an element at a specific index"},{"location":"lab1-introductory-python/#combining-arrays","text":"zip can be used to combine two or more arrays. # in editor joint_arr = list ( zip ([ 1 , 2 , 3 ], [ 'a' , 'b' , 'c' ])) # in IPython console joint_arr # [(1,'a'),(2,'b'),(3,'c')]","title":"Combining arrays"},{"location":"lab1-introductory-python/#loops","text":"Python supports for and while loops. To loop through every element in the array arr and print them to the console, for a in arr : print ( a ) # 1 # a # 2 # ... for a in arr : loops through all elements in arr and in each loop, an element in arr is assigned to the variable a . Note that in most other programming languages, code blocks are separated with delimiters such as the curly brackets ( {} ). This is not the case in Python. Code blocks in Python are defined by their indentation and normally initiated with a colon( : ). For example, for a in arr : print ( a ) print ( arr ) print(a) is the command to be executed in each loop. print(arr) is only executed after the for loop is completed. for a in arr : print ( a ) print ( arr ) In this case, print(arr) is executed in each loop. Using zip we can loop through two arrays at once. for item in zip ([ 'a' , 'b' , 'c' , 'd' ],[ 'artificial' , 'breadth' , 'cost' , 'depth' ]): print ( item [ 0 ] + ' for ' + item [ 1 ]) enumerate is useful in obtaining the index of the element in the array. for ( index , item ) in enumerate ([ 'a' , 'b' , 'c' , 'd' ]): print ( 'Index of ' + item + ' is: ' + str ( index )) Looping through a dictionary ( dict ) can also be done easily. x_dict = { 'd' : 'depth' , 'e' : 'estimation' , 'f' : 'frontier' } for key , value in x_dict . items (): print ( key + ' for ' + value ) while loops can be defined similarly. x = 0 while x != 10 : x += 1 print ( x ) print ( 'while loop is completed' ) The following example introduces nested array and multiple assignments. arr = [[ 'a' , 1 ],[ 'b' , 2 ],[ 'c' , 3 ],[ 'd' , 4 ],[ 'e' , 5 ],[ 'f' , 6 ]] for [ a , n ] in arr : print ( str ( a ) + ' is ' + str ( n ))","title":"Loops"},{"location":"lab1-introductory-python/#conditions","text":"The following operators can be used for conditional testing: Operator Definition == Equivalence != Inequivalence < Less than <= Less than or equal to > Greater than >= Greater than or equal to Python also supports text operator for conditional testing: Operator Definition Example (symbolic) Example (text) is Equivalence a == 1 a is 1 not Inequivalence a != 1 a is not 1 or not (a is 1) or not a is 1 or not(a == 1) Combining two conditions can also be done with text operators and and or . Symbolic operator Text operator & and | or","title":"Conditions"},{"location":"lab1-introductory-python/#user-defined-functions","text":"To define a custom function with the name of custom_fcn , def custom_fcn (): print ( 'This is a custom function to display custom message' ) To call the function, custom_fcn () # This is a custom function to display custom message","title":"User-defined functions"},{"location":"lab1-introductory-python/#user-defined-functions-with-input-arguments","text":"To define a custom function with input arguments, def custom_fcn ( msg ): print ( 'This is a custom function to display ' + msg ) The function can be called by custom_fcn ( 'new message' ) # This is a custom function to display new message","title":"User-defined functions with input arguments"},{"location":"lab1-introductory-python/#user-defined-functions-with-optional-input-arguments","text":"To define a custom function with optional input arguments, we just need to provide the default value to the optional input arguments. def custom_fcn ( msg = 'default message' ): print ( 'This is a custom function to display ' + msg ) The input arguments can also be specified as named inputs. custom_fcn ( msg = 'new message' ) # This is a custom function to display new message","title":"User-defined functions with optional input arguments"},{"location":"lab1-introductory-python/#exercise","text":"Create a new file in Spyder. Define a variable named friends such that it is a nested array in which contains the name, home country, and home state/province of 10 of your friends (real or virtual). For example, friends = [[ \"James\" , \"Malaysia\" , \"Malacca\" ], [ \"Goh\" , \"Australia\" , \"Brisbane\" ], [ \"Don\" , \"Malaysia\" , \"Pahang\" ]] Create a function in the same file with three (3) optional input arguments, name , home_country , home_state . def filterFriend ( name = \"\" , home_country = \"\" , home_state = \"\" ): ... return filtered This function will filter friends based on the input arguments provided. The function will ignore the input argument if it has empty string, i.e. \"\" . If any of the input arguments is provided, the function will find the friends whose detail(s) matches the input. For example, filterFriend(name=\"James\") will return [[\"James\", \"Malaysia\", \"Malacca\"]] filterFriend(home_country=\"Malaysia\") will return [[\"James\", \"Malaysia\", \"Malacca\"], [\"Don\", \"Malaysia\", \"Pahang\"]] .","title":"Exercise"},{"location":"lab2-fuzzy/","text":"Lab 2: Fuzzy Systems Lab learning outcomes After completing this lab, the students are able to construct a Mamdani fuzzy system using the scikit-fuzzy Python library and evaluate the result of the constructed fuzzy system. Note Install the scikit-fuzzy Python library in your environment before proceeding with the lab. conda install - c conda - forge scikit - fuzzy Fuzzy control system for a train Consider a fuzzy control system to control the brake and throttle of a train based on the speed of the train and the distance of the train to the next stop. Import the skfuzzy , skfuzzy.control , and numpy . import numpy as np from skfuzzy import control as ctrl from skfuzzy import membership as mf Initialise inputs and outputs Speed and distance are the inputs of the system whereas brake and throttle are the outputs. The ranges for the variables are: Variable Range Speed 0 - 85 km/h Distance 0 - 3000 m Brake 0 - 100% Throttle 0 - 100% As the inputs will be the antecedents of the rules, construct the variables speed and distance as skfuzzy.control.Antecedent objects. speed = ctrl . Antecedent ( np . arange ( 0 , 85 , 0.1 ), 'speed' ) The initialisation function for skfuzzy.control.Antecedent object takes 2 arguments, the first is the universe of the variable, i.e. the values the variables can take, the second is the label of the variable. The initialisation function for skfuzzy.control.Consequent is similar. The label and the range of the variable can be accessed using .label and .universe respectively. Task : Initialise the variables distance as Antecedent object, and brake and throttle as Consequent objects. (Outputs of the system will be consequents of the rules) Define membership functions for fuzzy sets of variables The fit vectors of the fuzzy sets for the linguistic variables are given as follows: speed (0 to 85 km/h) Linguistic value Fit vector Stopped (1/0, 0/2) Very slow (0/1, 1/2.5, 0/4) Slow (0/2.5, 1/6.5, 0/10.5) Medium fast (0/6.5, 1/26.5, 0/46.5) Fast (0/26.5, 1/70, 1/85) distance (0 to 3000 m) Linguistic value Fit vector At (1/0, 0/2) Very near (0/1, 1/3, 0/5) Near (0/3, 1/101.5, 0/200) Medium far (0/100, 1/1550, 0/3000) Far (0/1500, 1/2250, 1/3000) brake (0 to 100%) Linguistic value Fit vector No (1/0, 0/40) Very slight (0/20, 1/50, 0/80) Slight (0/70, 1/83.5, 0/97) Medium (0/95, 1/97, 0/99) Full (0/98, 1/100) throttle (0 to 100%) Linguistic value Fit vector No (1/0, 0/2) Very slight (0/1, 1/3, 0/5) Slight (0/3, 1/16.5, 0/30) Medium (0/20, 1/50, 0/80) Full (0/60, 1/80, 1/100) The skfuzzy.membership module provides the following membership functions: Membership function Description skfuzzy.membership.dsigmf(x, b1, c1, b2, c2) Difference of two fuzzy sigmoid membership functions skfuzzy.membership.gauss2mf(x, mean1, ...) Gaussian fuzzy membership function of two combined Gaussians skfuzzy.membership.gaussmf(x, mean, sigma) Gaussian fuzzy membership function skfuzzy.membership.gbellmf(x, a, b, c) Generalized Bell function fuzzy membership generator skfuzzy.membership.piecemf(x, abc) Piecewise linear membership function (particularly used in FIRE filters) skfuzzy.membership.pimf(x, a, b, c, d) Pi-function fuzzy membership generator skfuzzy.membership.psigmf(x, b1, c1, b2, c2) Product of two sigmoid membership functions skfuzzy.membership.sigmf(x, b, c) The basic sigmoid membership function generator skfuzzy.membership.smf(x, a, b) S-function fuzzy membership generator skfuzzy.membership.trapmf(x, abcd) Trapezoidal membership function generator skfuzzy.membership.trimf(x, abc) Triangular membership function generator skfuzzy.membership.zmf(x, a, b) Z-function fuzzy membership generator The fit vector of a linguitic value can be assigned to a linguistic variable using speed [ 'stopped' ] = mf . trimf ( speed . universe , [ 0 , 0 , 2 ]) speed [ 'very slow' ] = mf . trimf ( speed . universe , [ 1 , 2.5 , 4 ]) Task : Assign all fuzzy sets to the linguistic variables. The fuzzy set diagram of a linguistic variable can be viewed using .view() speed . view () Task : Check if the fuzzy set diagrams match the fit vectors. Define rules The rules for this system are displayed in the following fuzzy association memory (FAM) representaion table. Distance At Very near Near Medium far Far Speed Stopped Full brake No throttle Full brake Very slight throttle Very slow Full brake No throttle Medium brake Very slight throttle Slight brake Very slight throttle Slow Full brake No throttle Medium brake Very slight throttle Very slight brake Slight throttle Medium fast Very slight brake Medium throttle No brake Full throttle Fast Very slight brake Medium throttle No brake Full throttle Rule can be defined using skfuzzy.control.Rule(antecedent, consequent, label) . To define the first rule, i.e. if distance is 'at' and speed is 'stopped', then full brake and no throttle, rule1 = ctrl . Rule ( distance [ 'at' ] & speed [ 'stopped' ], ( brake [ 'full' ], throttle [ 'no' ])) If the antecedent consists of multiple parts, they can be combined using operators | (OR), & (AND), and ~ (NOT). If the consequent consists of multiple parts, they can be combined as a list / tuple . Task : Define all the rules. Then combine all the rules in a list , i.e. rules = [rule1, rule2, ...] . Construct the fuzzy control system The train control system can be constructed with train_ctrl = ctrl . ControlSystem ( rules = rules ) A skfuzzy.control.ControlSystemSimulation object is needed to simulate the control system to obtain the outputs given certain inputs. train = ctrl . ControlSystemSimulation ( control_system = train_ctrl ) To obtain the values for brake and throttle given that speed is 30 km/h and distance is 6 m, # define the values for the inputs train . input [ 'speed' ] = 30 train . input [ 'distance' ] = 2000 # compute the outputs train . compute () # print the output values print ( train . output ) # to extract one of the outputs print ( train . output [ 'brake' ]) To view the results in the graph, brake . view ( sim = train ) throttle . view ( sim = train ) View the control/output space The control/output space allows us to identify if the outputs fit our expectation. Construct an empty 3D space with 100-by-100 x-y grid. x , y = np . meshgrid ( np . linspace ( speed . universe . min (), speed . universe . max (), 100 ), np . linspace ( distance . universe . min (), distance . universe . max (), 100 )) z_brake = np . zeros_like ( x , dtype = float ) z_throttle = np . zeros_like ( x , dtype = float ) Loop through every point and identify the value of brake and throttle of each point. As the specified rules are not exhaustive, i.e. some input combinations do not activate any rule, we will set the output of such input combinations to be float('inf') . for i , r in enumerate ( x ): for j , c in enumerate ( r ): train . input [ 'speed' ] = x [ i , j ] train . input [ 'distance' ] = y [ i , j ] try : train . compute () except : z_brake [ i , j ] = float ( 'inf' ) z_throttle [ i , j ] = float ( 'inf' ) z_brake [ i , j ] = train . output [ 'brake' ] z_throttle [ i , j ] = train . output [ 'throttle' ] Plot the result in a 3D graph using the matplotlib.pyplot library. import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D def plot3d ( x , y , z ): fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . plot_surface ( x , y , z , rstride = 1 , cstride = 1 , cmap = 'viridis' , linewidth = 0.4 , antialiased = True ) ax . contourf ( x , y , z , zdir = 'z' , offset =- 2.5 , cmap = 'viridis' , alpha = 0.5 ) ax . contourf ( x , y , z , zdir = 'x' , offset = x . max () * 1.5 , cmap = 'viridis' , alpha = 0.5 ) ax . contourf ( x , y , z , zdir = 'y' , offset = y . max () * 1.5 , cmap = 'viridis' , alpha = 0.5 ) ax . view_init ( 30 , 200 ) plot3d ( x , y , z_brake ) plot3d ( x , y , z_throttle ) Fuzzy tipping recommendation system A fuzzy expert system is designed to identify the percentage of tips a customer will give based on the service and the food the customer received. The system has service rating and food rating as inputs, and tips as output. The fit vectors of the fuzzy sets for the linguistic variables are given as follows: service (0 to 10) Linguistic value Fit vector Poor (1/0, 0/5) Average (0/0, 1/5, 0/10) Good (0/5, 1/10) food (0 to 10) Linguistic value Fit vector Poor (1/0, 0/5) Average (0/0, 1/5, 0/10) Good (0/5, 1/10) tips (0 to 30%) Linguistic value Fit vector Low (1/0, 0/15) Medium (0/0, 1/15, 0/30) High (0/15, 1/30) The rules are displayed in the following fuzzy association memory (FAM) representaion table. Food Poor Average Good Service Poor low tips low tips medium tips Average low tips medium tips high tips Good medium tips high tips high tips Task : Construct the fuzzy inference system. Task : Discuss, if you were to make at least one modification to the fuzzy tipping recommendation system, what it will be and why. Report Submit a report detailing the process, results, graphs, and your observations.","title":"Lab 2: Fuzzy Systems"},{"location":"lab2-fuzzy/#lab-2-fuzzy-systems","text":"","title":"Lab 2: Fuzzy Systems"},{"location":"lab2-fuzzy/#lab-learning-outcomes","text":"After completing this lab, the students are able to construct a Mamdani fuzzy system using the scikit-fuzzy Python library and evaluate the result of the constructed fuzzy system.","title":"Lab learning outcomes"},{"location":"lab2-fuzzy/#note","text":"Install the scikit-fuzzy Python library in your environment before proceeding with the lab. conda install - c conda - forge scikit - fuzzy","title":"Note"},{"location":"lab2-fuzzy/#fuzzy-control-system-for-a-train","text":"Consider a fuzzy control system to control the brake and throttle of a train based on the speed of the train and the distance of the train to the next stop. Import the skfuzzy , skfuzzy.control , and numpy . import numpy as np from skfuzzy import control as ctrl from skfuzzy import membership as mf","title":"Fuzzy control system for a train"},{"location":"lab2-fuzzy/#initialise-inputs-and-outputs","text":"Speed and distance are the inputs of the system whereas brake and throttle are the outputs. The ranges for the variables are: Variable Range Speed 0 - 85 km/h Distance 0 - 3000 m Brake 0 - 100% Throttle 0 - 100% As the inputs will be the antecedents of the rules, construct the variables speed and distance as skfuzzy.control.Antecedent objects. speed = ctrl . Antecedent ( np . arange ( 0 , 85 , 0.1 ), 'speed' ) The initialisation function for skfuzzy.control.Antecedent object takes 2 arguments, the first is the universe of the variable, i.e. the values the variables can take, the second is the label of the variable. The initialisation function for skfuzzy.control.Consequent is similar. The label and the range of the variable can be accessed using .label and .universe respectively. Task : Initialise the variables distance as Antecedent object, and brake and throttle as Consequent objects. (Outputs of the system will be consequents of the rules)","title":"Initialise inputs and outputs"},{"location":"lab2-fuzzy/#define-membership-functions-for-fuzzy-sets-of-variables","text":"The fit vectors of the fuzzy sets for the linguistic variables are given as follows: speed (0 to 85 km/h) Linguistic value Fit vector Stopped (1/0, 0/2) Very slow (0/1, 1/2.5, 0/4) Slow (0/2.5, 1/6.5, 0/10.5) Medium fast (0/6.5, 1/26.5, 0/46.5) Fast (0/26.5, 1/70, 1/85) distance (0 to 3000 m) Linguistic value Fit vector At (1/0, 0/2) Very near (0/1, 1/3, 0/5) Near (0/3, 1/101.5, 0/200) Medium far (0/100, 1/1550, 0/3000) Far (0/1500, 1/2250, 1/3000) brake (0 to 100%) Linguistic value Fit vector No (1/0, 0/40) Very slight (0/20, 1/50, 0/80) Slight (0/70, 1/83.5, 0/97) Medium (0/95, 1/97, 0/99) Full (0/98, 1/100) throttle (0 to 100%) Linguistic value Fit vector No (1/0, 0/2) Very slight (0/1, 1/3, 0/5) Slight (0/3, 1/16.5, 0/30) Medium (0/20, 1/50, 0/80) Full (0/60, 1/80, 1/100) The skfuzzy.membership module provides the following membership functions: Membership function Description skfuzzy.membership.dsigmf(x, b1, c1, b2, c2) Difference of two fuzzy sigmoid membership functions skfuzzy.membership.gauss2mf(x, mean1, ...) Gaussian fuzzy membership function of two combined Gaussians skfuzzy.membership.gaussmf(x, mean, sigma) Gaussian fuzzy membership function skfuzzy.membership.gbellmf(x, a, b, c) Generalized Bell function fuzzy membership generator skfuzzy.membership.piecemf(x, abc) Piecewise linear membership function (particularly used in FIRE filters) skfuzzy.membership.pimf(x, a, b, c, d) Pi-function fuzzy membership generator skfuzzy.membership.psigmf(x, b1, c1, b2, c2) Product of two sigmoid membership functions skfuzzy.membership.sigmf(x, b, c) The basic sigmoid membership function generator skfuzzy.membership.smf(x, a, b) S-function fuzzy membership generator skfuzzy.membership.trapmf(x, abcd) Trapezoidal membership function generator skfuzzy.membership.trimf(x, abc) Triangular membership function generator skfuzzy.membership.zmf(x, a, b) Z-function fuzzy membership generator The fit vector of a linguitic value can be assigned to a linguistic variable using speed [ 'stopped' ] = mf . trimf ( speed . universe , [ 0 , 0 , 2 ]) speed [ 'very slow' ] = mf . trimf ( speed . universe , [ 1 , 2.5 , 4 ]) Task : Assign all fuzzy sets to the linguistic variables. The fuzzy set diagram of a linguistic variable can be viewed using .view() speed . view () Task : Check if the fuzzy set diagrams match the fit vectors.","title":"Define membership functions for fuzzy sets of variables"},{"location":"lab2-fuzzy/#define-rules","text":"The rules for this system are displayed in the following fuzzy association memory (FAM) representaion table. Distance At Very near Near Medium far Far Speed Stopped Full brake No throttle Full brake Very slight throttle Very slow Full brake No throttle Medium brake Very slight throttle Slight brake Very slight throttle Slow Full brake No throttle Medium brake Very slight throttle Very slight brake Slight throttle Medium fast Very slight brake Medium throttle No brake Full throttle Fast Very slight brake Medium throttle No brake Full throttle Rule can be defined using skfuzzy.control.Rule(antecedent, consequent, label) . To define the first rule, i.e. if distance is 'at' and speed is 'stopped', then full brake and no throttle, rule1 = ctrl . Rule ( distance [ 'at' ] & speed [ 'stopped' ], ( brake [ 'full' ], throttle [ 'no' ])) If the antecedent consists of multiple parts, they can be combined using operators | (OR), & (AND), and ~ (NOT). If the consequent consists of multiple parts, they can be combined as a list / tuple . Task : Define all the rules. Then combine all the rules in a list , i.e. rules = [rule1, rule2, ...] .","title":"Define rules"},{"location":"lab2-fuzzy/#construct-the-fuzzy-control-system","text":"The train control system can be constructed with train_ctrl = ctrl . ControlSystem ( rules = rules ) A skfuzzy.control.ControlSystemSimulation object is needed to simulate the control system to obtain the outputs given certain inputs. train = ctrl . ControlSystemSimulation ( control_system = train_ctrl ) To obtain the values for brake and throttle given that speed is 30 km/h and distance is 6 m, # define the values for the inputs train . input [ 'speed' ] = 30 train . input [ 'distance' ] = 2000 # compute the outputs train . compute () # print the output values print ( train . output ) # to extract one of the outputs print ( train . output [ 'brake' ]) To view the results in the graph, brake . view ( sim = train ) throttle . view ( sim = train )","title":"Construct the fuzzy control system"},{"location":"lab2-fuzzy/#view-the-controloutput-space","text":"The control/output space allows us to identify if the outputs fit our expectation. Construct an empty 3D space with 100-by-100 x-y grid. x , y = np . meshgrid ( np . linspace ( speed . universe . min (), speed . universe . max (), 100 ), np . linspace ( distance . universe . min (), distance . universe . max (), 100 )) z_brake = np . zeros_like ( x , dtype = float ) z_throttle = np . zeros_like ( x , dtype = float ) Loop through every point and identify the value of brake and throttle of each point. As the specified rules are not exhaustive, i.e. some input combinations do not activate any rule, we will set the output of such input combinations to be float('inf') . for i , r in enumerate ( x ): for j , c in enumerate ( r ): train . input [ 'speed' ] = x [ i , j ] train . input [ 'distance' ] = y [ i , j ] try : train . compute () except : z_brake [ i , j ] = float ( 'inf' ) z_throttle [ i , j ] = float ( 'inf' ) z_brake [ i , j ] = train . output [ 'brake' ] z_throttle [ i , j ] = train . output [ 'throttle' ] Plot the result in a 3D graph using the matplotlib.pyplot library. import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D def plot3d ( x , y , z ): fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . plot_surface ( x , y , z , rstride = 1 , cstride = 1 , cmap = 'viridis' , linewidth = 0.4 , antialiased = True ) ax . contourf ( x , y , z , zdir = 'z' , offset =- 2.5 , cmap = 'viridis' , alpha = 0.5 ) ax . contourf ( x , y , z , zdir = 'x' , offset = x . max () * 1.5 , cmap = 'viridis' , alpha = 0.5 ) ax . contourf ( x , y , z , zdir = 'y' , offset = y . max () * 1.5 , cmap = 'viridis' , alpha = 0.5 ) ax . view_init ( 30 , 200 ) plot3d ( x , y , z_brake ) plot3d ( x , y , z_throttle )","title":"View the control/output space"},{"location":"lab2-fuzzy/#fuzzy-tipping-recommendation-system","text":"A fuzzy expert system is designed to identify the percentage of tips a customer will give based on the service and the food the customer received. The system has service rating and food rating as inputs, and tips as output. The fit vectors of the fuzzy sets for the linguistic variables are given as follows: service (0 to 10) Linguistic value Fit vector Poor (1/0, 0/5) Average (0/0, 1/5, 0/10) Good (0/5, 1/10) food (0 to 10) Linguistic value Fit vector Poor (1/0, 0/5) Average (0/0, 1/5, 0/10) Good (0/5, 1/10) tips (0 to 30%) Linguistic value Fit vector Low (1/0, 0/15) Medium (0/0, 1/15, 0/30) High (0/15, 1/30) The rules are displayed in the following fuzzy association memory (FAM) representaion table. Food Poor Average Good Service Poor low tips low tips medium tips Average low tips medium tips high tips Good medium tips high tips high tips Task : Construct the fuzzy inference system. Task : Discuss, if you were to make at least one modification to the fuzzy tipping recommendation system, what it will be and why.","title":"Fuzzy tipping recommendation system"},{"location":"lab2-fuzzy/#report","text":"Submit a report detailing the process, results, graphs, and your observations.","title":"Report"},{"location":"lab3-genetic-algorithm/","text":"Lab 3: Genetic Algorithm Lab learning outcomes After completing this lab, the students are able to implement genetic algorithm to solve an optimisation problem. Binary-to-gray code conversion Binary string is often used in the implementation of genetic algorithm. However, the downside of using a binary code is that the Hamming distance between two adjacent values is not consistent. This situation is solved by using a Gray code in place of a binary code. numpy provides the function of binary_repr to convert a decimal value to its corresponding binary code. Create a function to take the input of a binary code and return the correponding Gray code of the binary code. Create a function to calculate the Hamming distance between two binary strings (two binary codes or two Gray codes). Consider a sequence of decimal values of [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] . Convert the sequence to a series of binary codes. Identify and plot ( example of a line plot ) the Hamming distances between the adjacent values. Repeat the previous step with Gray codes instead of binary codes. Genetic algorithm Consider the following problem: Problem You are given a sheet of paper with width w and height h . Your task is to cut the paper into squares of equal size. The aim of the task is to have as many squares as possible, and to have the area of each square as large as possible. An optimisation problem can always be phrased in the form of to optimise ... such that it maximises/minimises ... In this problem, what is the parameter to be optimised and what are the parameters to be maximised or minimised? Let x denotes the length of the sides of a square. We need a fitness function such that higher fitness corresponds to larger number of squares and large area. If the number of squares (that can be cut out) is zero, or the area of the square is zero, the fitness will be zero. Consider the fitness function of number of squares * area of a single square . feature encoding population initialisation selection as parents crossover mutation offspring (next generation population) repeat from fitnexx calculation until termination Feature encoding In this problem as we only have one feature, i.e. the side length of the square, each chromosome consists of the value of the side length of the square. We will encode the chromosome in the form of Gray code. Create two functions value2gray and gray2value to convert a decimal value to its Gray code and vice versa. def value2gray ( value ): # this function converts a decimal value to its gray code representation ... return gray def gray2value ( gray ): # this function converts a gray code representation to its decimal value ... return value Add the following code snippet to the end of the code to test your functions. if __name__ == \"__main__\" : print ( value2gray ( 10 )) print ( gray2value ( \"1001\" )) After running the file as a script, you should see the following output. 1111 14 Population initialisation A population is randomly generated according to the defined population size. Create a function to generate randomly a population of size pop_size with each value lies between the range of pop_min to pop_max . def generatePopulation ( pop_size , pop_min , pop_max ): # this function generate the first generation randomly based on the population size and the range of the value of each chromosome ... return population This function and all the functions created after this should be placed before the if __name__ == \"__main__\": code block. [Optional testing] You can test the function by changing the __main__ code block to if __name__ == \"__main__\" : print ( generatePopulation ( 8 , 0 , 10 )) The printed output should be a series of 8 chromosomes displayed as decimal values. Fitness calculation The fitness function was designed at the beginning of this section . Define a function that takes the input of a chromosome (as decimal value) and returns the fitness of the chromosome. def calculateFitness ( value ): # this function calculates the fitness of a chromosome from the decimal value of the chromosome ... return fitness 2. [Optional] Test the function with if __name__ == \"__main__\" : print ( calculateFitness ( 5 )) The printed output should be the fitness of a chromosome of value 5, which would be a decimal value larger than zero. Selection as parents From the list of the chromosomes, we will select the chromosome pairs as parents. As we will be using one-point crossover, each pair of parents will produce exactly two offsprings. Therefore for population size of pop_size , we need pop_size/2 pairs of parents. Define a function that takes the inputs of the current population and the total number of chromosomes in current population, and returns the chromosome pairs which will act as parents. The selection process is performed with the roulette wheel selection. The same chromosome can be selected more than once. def selectParents ( chromosomes , pop_size ): ... return parent_pairs [Optional] Test the function with if __name__ == \"__main__\" : print ( selectParents ([ 13 , 8 , 14 , 7 ], 6 )) The printed output should be 3 parent pairs, for example, [[13, 8], [8, 14], [13, 7]] Crossover Define a function that takes a parent pair and returns a pair of offspring after performing one-point crossover. def crossover ( parents ): # this function takes a parent pair and perform one-point crossover to produce a pair of offspring ... return offsprings [Optional] Test the function with if __name__ == \"__main__\" : print ( crossover ([ 13 , 9 ])) The printed output should be a pair of offsprings, for example, [10, 14] 13 is 1011 and 9 is 1101 in Gray code, the offsprings 10 is 1111 and 14 is 1001 in Gray code. Mutation Each gene in all chromosomes has the same mutation probability p_mutation . Define a function that takes a chromosome and the mutation probability p_mutation as the inputs, and returns the mutated chromosome. def mutate ( chromosome , p_mutation ): # this function mutates each gene of a chromosome based on the mutation probability ... return mutated 3. [Optional] Test the function with if __name__ == \"__main__\" : print ( mutate ( 15 , 0.1 )) The printed output should be the mutated or unmutated chromosome, for example, 14 . 15 is 1000 and 14 is 1001 in Gray code. In the example output, the last bit is mutated. Repeat until termination The common termination criteria are the maximum number of iterations and the distance among the fitnesses of the chromosomes of the latest population. Define a function that calculates one metric to measure the distance among the fitnesses of the chromosomes, i.e. how far the fitnesses of all the chromosomes are from each other. def findOverallDistance ( chromosomes ): # this function takes the input of the current population and returns the overall distance among fitnesses of all chromosomes ... return overall_distance [Optional] Test the function with if __name__ == \"__main__\" : print ( findOverallDistance ([ 13 , 11 , 14 , 7 ])) The printed output should be a decimal value that represents the overall distance of fitnesses. Combining all functions The functions we have created can be combined with the following code snippet to execute the genetic algorithm to solve the problem defined at the beginning of this section . Consider the width and the height of the sheet of paper to be 20cm and 15cm . if __name__ == \"__main__\" : # main function ## parameter definition pop_size = 10 pop_min = 1 #1cm pop_max = 10 #10cm curr_iter = 0 max_iter = 100 min_overalldistance = 0.5 p_mutation = 0.05 ## initialise population population = [] population . append ( generatePopulation ( pop_size , pop_min , pop_max )) while ( curr_iter < max_iter and findOverallDistance ( population [ - 1 ]) > min_overalldistance ): curr_iter += 1 ## select parent pairs parents = selectParents ( population [ - 1 ], len ( population [ - 1 ])) ## perform crossover offsprings = [] for p in parents : new_offsprings = crossover ( p ) for o in new_offsprings : offsprings . append ( o ) ## perform mutation mutated = [ mutate ( offspring , p_mutation ) for offspring in offsprings ] ## update current population population . append ( mutated ) Task : Consider a different fitness function than the one defined here . How does it change the outcome of the algorithm? Report Submit a report detailing the process, results, graphs, and your observations.","title":"Lab 3: Genetic Algorithm"},{"location":"lab3-genetic-algorithm/#lab-3-genetic-algorithm","text":"","title":"Lab 3: Genetic Algorithm"},{"location":"lab3-genetic-algorithm/#lab-learning-outcomes","text":"After completing this lab, the students are able to implement genetic algorithm to solve an optimisation problem.","title":"Lab learning outcomes"},{"location":"lab3-genetic-algorithm/#binary-to-gray-code-conversion","text":"Binary string is often used in the implementation of genetic algorithm. However, the downside of using a binary code is that the Hamming distance between two adjacent values is not consistent. This situation is solved by using a Gray code in place of a binary code. numpy provides the function of binary_repr to convert a decimal value to its corresponding binary code. Create a function to take the input of a binary code and return the correponding Gray code of the binary code. Create a function to calculate the Hamming distance between two binary strings (two binary codes or two Gray codes). Consider a sequence of decimal values of [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] . Convert the sequence to a series of binary codes. Identify and plot ( example of a line plot ) the Hamming distances between the adjacent values. Repeat the previous step with Gray codes instead of binary codes.","title":"Binary-to-gray code conversion"},{"location":"lab3-genetic-algorithm/#genetic-algorithm","text":"Consider the following problem: Problem You are given a sheet of paper with width w and height h . Your task is to cut the paper into squares of equal size. The aim of the task is to have as many squares as possible, and to have the area of each square as large as possible. An optimisation problem can always be phrased in the form of to optimise ... such that it maximises/minimises ... In this problem, what is the parameter to be optimised and what are the parameters to be maximised or minimised? Let x denotes the length of the sides of a square. We need a fitness function such that higher fitness corresponds to larger number of squares and large area. If the number of squares (that can be cut out) is zero, or the area of the square is zero, the fitness will be zero. Consider the fitness function of number of squares * area of a single square . feature encoding population initialisation selection as parents crossover mutation offspring (next generation population) repeat from fitnexx calculation until termination","title":"Genetic algorithm"},{"location":"lab3-genetic-algorithm/#feature-encoding","text":"In this problem as we only have one feature, i.e. the side length of the square, each chromosome consists of the value of the side length of the square. We will encode the chromosome in the form of Gray code. Create two functions value2gray and gray2value to convert a decimal value to its Gray code and vice versa. def value2gray ( value ): # this function converts a decimal value to its gray code representation ... return gray def gray2value ( gray ): # this function converts a gray code representation to its decimal value ... return value Add the following code snippet to the end of the code to test your functions. if __name__ == \"__main__\" : print ( value2gray ( 10 )) print ( gray2value ( \"1001\" )) After running the file as a script, you should see the following output. 1111 14","title":"Feature encoding"},{"location":"lab3-genetic-algorithm/#population-initialisation","text":"A population is randomly generated according to the defined population size. Create a function to generate randomly a population of size pop_size with each value lies between the range of pop_min to pop_max . def generatePopulation ( pop_size , pop_min , pop_max ): # this function generate the first generation randomly based on the population size and the range of the value of each chromosome ... return population This function and all the functions created after this should be placed before the if __name__ == \"__main__\": code block. [Optional testing] You can test the function by changing the __main__ code block to if __name__ == \"__main__\" : print ( generatePopulation ( 8 , 0 , 10 )) The printed output should be a series of 8 chromosomes displayed as decimal values.","title":"Population initialisation"},{"location":"lab3-genetic-algorithm/#fitness-calculation","text":"The fitness function was designed at the beginning of this section . Define a function that takes the input of a chromosome (as decimal value) and returns the fitness of the chromosome. def calculateFitness ( value ): # this function calculates the fitness of a chromosome from the decimal value of the chromosome ... return fitness 2. [Optional] Test the function with if __name__ == \"__main__\" : print ( calculateFitness ( 5 )) The printed output should be the fitness of a chromosome of value 5, which would be a decimal value larger than zero.","title":"Fitness calculation"},{"location":"lab3-genetic-algorithm/#selection-as-parents","text":"From the list of the chromosomes, we will select the chromosome pairs as parents. As we will be using one-point crossover, each pair of parents will produce exactly two offsprings. Therefore for population size of pop_size , we need pop_size/2 pairs of parents. Define a function that takes the inputs of the current population and the total number of chromosomes in current population, and returns the chromosome pairs which will act as parents. The selection process is performed with the roulette wheel selection. The same chromosome can be selected more than once. def selectParents ( chromosomes , pop_size ): ... return parent_pairs [Optional] Test the function with if __name__ == \"__main__\" : print ( selectParents ([ 13 , 8 , 14 , 7 ], 6 )) The printed output should be 3 parent pairs, for example, [[13, 8], [8, 14], [13, 7]]","title":"Selection as parents"},{"location":"lab3-genetic-algorithm/#crossover","text":"Define a function that takes a parent pair and returns a pair of offspring after performing one-point crossover. def crossover ( parents ): # this function takes a parent pair and perform one-point crossover to produce a pair of offspring ... return offsprings [Optional] Test the function with if __name__ == \"__main__\" : print ( crossover ([ 13 , 9 ])) The printed output should be a pair of offsprings, for example, [10, 14] 13 is 1011 and 9 is 1101 in Gray code, the offsprings 10 is 1111 and 14 is 1001 in Gray code.","title":"Crossover"},{"location":"lab3-genetic-algorithm/#mutation","text":"Each gene in all chromosomes has the same mutation probability p_mutation . Define a function that takes a chromosome and the mutation probability p_mutation as the inputs, and returns the mutated chromosome. def mutate ( chromosome , p_mutation ): # this function mutates each gene of a chromosome based on the mutation probability ... return mutated 3. [Optional] Test the function with if __name__ == \"__main__\" : print ( mutate ( 15 , 0.1 )) The printed output should be the mutated or unmutated chromosome, for example, 14 . 15 is 1000 and 14 is 1001 in Gray code. In the example output, the last bit is mutated.","title":"Mutation"},{"location":"lab3-genetic-algorithm/#repeat-until-termination","text":"The common termination criteria are the maximum number of iterations and the distance among the fitnesses of the chromosomes of the latest population. Define a function that calculates one metric to measure the distance among the fitnesses of the chromosomes, i.e. how far the fitnesses of all the chromosomes are from each other. def findOverallDistance ( chromosomes ): # this function takes the input of the current population and returns the overall distance among fitnesses of all chromosomes ... return overall_distance [Optional] Test the function with if __name__ == \"__main__\" : print ( findOverallDistance ([ 13 , 11 , 14 , 7 ])) The printed output should be a decimal value that represents the overall distance of fitnesses.","title":"Repeat until termination"},{"location":"lab3-genetic-algorithm/#combining-all-functions","text":"The functions we have created can be combined with the following code snippet to execute the genetic algorithm to solve the problem defined at the beginning of this section . Consider the width and the height of the sheet of paper to be 20cm and 15cm . if __name__ == \"__main__\" : # main function ## parameter definition pop_size = 10 pop_min = 1 #1cm pop_max = 10 #10cm curr_iter = 0 max_iter = 100 min_overalldistance = 0.5 p_mutation = 0.05 ## initialise population population = [] population . append ( generatePopulation ( pop_size , pop_min , pop_max )) while ( curr_iter < max_iter and findOverallDistance ( population [ - 1 ]) > min_overalldistance ): curr_iter += 1 ## select parent pairs parents = selectParents ( population [ - 1 ], len ( population [ - 1 ])) ## perform crossover offsprings = [] for p in parents : new_offsprings = crossover ( p ) for o in new_offsprings : offsprings . append ( o ) ## perform mutation mutated = [ mutate ( offspring , p_mutation ) for offspring in offsprings ] ## update current population population . append ( mutated ) Task : Consider a different fitness function than the one defined here . How does it change the outcome of the algorithm?","title":"Combining all functions"},{"location":"lab3-genetic-algorithm/#report","text":"Submit a report detailing the process, results, graphs, and your observations.","title":"Report"},{"location":"lab4-particle-swarm-optimisation/","text":"Lab 4: Particle Swarm Optimisastion Lab learning outcomes After completing this lab, the students are able to develop a Python function to perform global best particle swarm optimisation. Setup for Spyder If you are using Spyder for this lab, go to Tools > Preferences > IPython console > Graphics and set Backend to Automatic . Restart kernel by going to Consoles > Restart kernel . Problem to solve Solve the following problem using global best particle swarm optimisation: Problem Find the value of x to minimise the function \\(f(x) = (x+100)(x+50)(x)(x-20)(x-60)(x-100)\\) for \\(-100 < x < 100\\) Particle swarm optimisation particles initialisation personal best identification global best identification velocity calculation position update repeat from personal best identification until termination Parameter definition With global best particle swarm optimisaton, the position update function is given by \\[x_i(t+1) = x_i(t) + v_i(t+1)\\] and the velocity update function is \\[v_i(t+1) = v_i(t) + \\alpha_1\\beta_1(t) \\Big( p_i(t) - x_i(t) \\Big) + \\alpha_2\\beta_2(t)\\Big(p_g(t) - x_i(t)\\Big)\\] \u03b1 1 and \u03b1 2 are acceleration constants that are fixed throughout the algorithm. Define a small value for \u03b1 1 and \u03b1 2 , for example 0.1 . alpha = [ 0.1 , 0.1 ] \u03b2 1 (t) and \u03b2 2 (t) are random values between 0 and 1 that are regenerated every iteration. Therefore no definition is required. Also, define the number of particles to run the algorithm with. n_particle = 10 Place the definition of these variables in the __main__ block. if __name__ == '__main__' : alpha = [ 0.1 , 0.1 ] n_particle = 10 Create a class for particle As each particle is an individual, create a Particle class to hold the data of the particle's current position, velocity, and personal best position. class Particle : def __init__ ( self , position = 0 , velocity = 0 ): self . position = position self . velocity = velocity self . best_position = position Fitness function Fitness function is how we can compare different particles. As our goal is to minimise f(x) as stated in the beginning , we will use f(x) as our fitness function. By using f(x) in minimisation problem, it implies that the lower the value of f(x), the better the particle it is. The value of x is the position of the particle. Define the fitness function as a Python function. def fit_fcn ( position ): ... return fitness Initialise particles Particles are initialised with random positions within the constraints. At initialisation, we may assume that the initial velocities of all the particles. It is possible to initialise particles with non-zero velocities. For now, we will stick to zero initial velocities. Define a Python function that takes the input of the number of particles and the limits of the positions to initialise and return a list of objects of class Particle . Each particle has random position within the limits and zero velocity. def initialise_particles ( n_ptc , position_limits ): # position_limits is a list of two values. The first value is the lower boundary and the second value is the upper boundary. ... return particles Remember to test your function before proceed. Update personal best Create a method in the class Particle to update the best_position if necessary. class Particle : def __init__ ( ... ): ... def update_personal_best ( self ): # 1. calculate the fitnesses of the best_position and the particle's current position # 2. compare the fitnesses and determine if the current position is better than the best_position # 3. update if necessary # 4. no return statement is required If the new position has a lower fitness, i.e. the new position is better than the best position, update the best_position to hold the value of the new position. Update global best Initiate a variable named global_best_position with the value None in the __main__ block. Create a function that takes two positions as inputs, compare them, and return the better position of the two. def compareFitness ( pos1 , pos2 ): # 1. calculate the fitness of pos1 and pos2 # 2. compare to determine the better position return betterpos We will later use this function to compare the current global best position with the personal best position of each particle. Update velocity Create a method in the class Particle to update the velocity given \u03b1 1 , \u03b1 2 , \u03b2 1 , \u03b2 2 , and the global best position. class Particle : def __init__ ( ... ): ... def update_personal_best ( ... ): ... def update_velocity ( self , alpha , beta , glob_best_pos ): # alpha is a list of two values. we will access alpha_1 and alpha_2 by alpha[0] and alpha[1] respectively. This also applies to beta. # the current position, current velocity, and personal best position of the particle can be accessed by self.position, self.velocity, and self.best_position # assign the particle's velocity with the updated velocity Update particle position As updating a particle position only require information from within the particle object and the limits of the position, create a method called update_position in the class Particle taking the input of the limits of the position. class Particle : def __init__ ( ... ): ... def update_personal_best ( ... ): ... def update_velocity ( ... ): ... def update_position ( self , position_limits ): self . position = self . position + self . velocity # how should you solve the problem of the position (x) going out of the limits Create a loop (until termination) Consider the following termination criteria: exceeding 200 iterations fitnesses of all particles are close positions of all particles are close Create a function to calculate the average difference between the mean fitness and the fitness of each particle. def calc_avg_fit_diff ( particles ): # 1. calculate mean fitness of all particles # 2. calculate the difference between the mean fitness and the fitness of each particle # 3. calculate the average of the differences obtained from step 2 return avg_fit_diff Create a function to calculate the average difference between the mean position and the position of each particle. def calc_avg_pos_diff ( particles ): # 1. calculate mean position of all particles # 2. calculate the difference between the mean position and the position of each particle # 3. calculate the average of the differences obtained from step 2 return avg_pos_diff Create a loop (in the __main__ block) to execute the global best particle swarm optimisation (gbest PSO) until termination. if __name__ == '__main__' : # parameter initialisation alpha = [ 0.1 , 0.1 ] n_particle = 10 global_best_position = None position_limits = [ - 100 , 100 ] # termination threshold iteration = 0 max_iter = 200 min_avg_fit_diff = 0.1 min_avg_pos_diff = 0.1 # initialise particles particles = initialise_particles ( n_particle , position_limits ) while ( ... ): # how should you define the termination criteria here? print ( iteration , [ round ( x . position , 2 ) for x in particles ]) for particle in particles : # update personal best particle . update_personal_best () # update global best if global_best_position == None : global_best_position = particle . position else : global_best_position = compareFitness ( global_best_position , particle . position ) # generate beta randomly for current iteration beta = [ random . random (), random . random ()] for particle in particles : # update velocity particle . update_velocity ( alpha , beta , global_best_position ) # update position particle . update_position ( position_limits ) iteration += 1 # display results print ( iteration , [ round ( x . position , 2 ) for x in particles ]) Visualisation Let's add a few lines to visualise particles \"flying\" towards to optimal position. import the visualisation library import matplotlib.pyplot as plt add the following lines just before the while loop in the last code block in the previous section . This is to plot the existing particle positions on the graph. space_ax = plt . axes () space_ax . plot ( list ( range ( * position_limits )),[ fit_fcn ( x ) for x in range ( * position_limits )]) space_ax . set_title ( \"Position of particles in iteration {} \" . format ( iteration )) space_ax . set_xlabel ( \"Position\" ) space_ax . set_ylabel ( \"Fitness\" ) add the following lines between line 14 and line 15 in the last code block in the previous section , as well as after line 33. This is to remove the existing particle positions and plot the new positions, i.e to visually update the positions. if len ( space_ax . lines ) > 1 : space_ax . lines [ 1 ] . remove () space_ax . plot ([ x . position for x in particles ], [ fit_fcn ( x . position ) for x in particles ], 'go' ) space_ax . set_title ( \"Position of particles in iteration {} \" . format ( iteration )) plt . pause ( 0.5 ) # pause the program for 0.5 second; if graph changes too quickly, increase this value; you can also speed up the process by decreasing this value Evaluation Store the values of the variables at each iteration for analysis and evaluation. position of each particle at each iteration (add the new line of code to the end of the methods) class Particle : def __init__ ( ... ): ... self . position_list = [ position ] def update_position ( ... ): ... self . position_list . append ( self . position ) velocity of each particle at each iteration (add the new line of code to the end of the methods) class Particle : def __init__ ( ... ): ... self . velocity_list = [ velocity ] def update_velocity ( ... ): ... self . velocity_list . append ( self . velocity ) personal best position of each particle at each iteration (add the new line of code to the end of the methods) class Particle : def __init__ ( ... ): ... self . best_position_list = [] def update_personal_best ( ... ): ... self . best_position_list . append ( self . best_position ) global best position at each iteration if __init__ == '__main__' : # parameter initialisation ... global_best_position_list = [] ... global_best_position = ... global_best_position_list . append ( global_best_position ) # take note on the indentation # generate beta randomly for current iteration ... Visualise the progression of these variables by adding the following code to the end of the __main__ block. [ pos_fig , position_axes ] = plt . subplots ( 4 , 1 , sharex = True ) position_axes [ 0 ] . set_title ( \"Position of each particle\" ) position_axes [ 1 ] . set_title ( \"Fitness of each particle\" ) position_axes [ 2 ] . set_title ( \"Boxplot of position at each iteration\" ) position_axes [ 3 ] . set_title ( \"Boxplot of fitness at each iteration\" ) position_axes [ 3 ] . set_xlabel ( \"Iteration\" ) [ vel_fig , velocity_axes ] = plt . subplots ( 2 , 1 , sharex = True ) velocity_axes [ 0 ] . set_title ( \"Velocity of each particle\" ) velocity_axes [ 1 ] . set_title ( \"Boxplot for velocity at each iteration\" ) velocity_axes [ 1 ] . set_xlabel ( \"Iteration\" ) [ p_best_fig , personal_best_axes ] = plt . subplots ( 4 , 1 , sharex = True ) personal_best_axes [ 0 ] . set_title ( \"Personal best position of each particle\" ) personal_best_axes [ 1 ] . set_title ( \"Personal best fitness of each particle\" ) personal_best_axes [ 2 ] . set_title ( \"Boxplot of personal best position at each iteration\" ) personal_best_axes [ 3 ] . set_title ( \"Boxplot of personal best fitness at each iteration\" ) personal_best_axes [ 3 ] . set_xlabel ( \"Iteration\" ) [ g_best_fig , global_best_axes ] = plt . subplots ( 2 , 1 , sharex = True ) global_best_axes [ 0 ] . set_title ( \"Global best position\" ) global_best_axes [ 1 ] . set_title ( \"Fitness for global best position\" ) global_best_axes [ 1 ] . set_xlabel ( \"Iteration\" ) for particle in particles : iteration_list = list ( range ( len ( particle . position_list ))) position_axes [ 0 ] . plot ( iteration_list , particle . position_list , '-o' ) position_axes [ 1 ] . plot ( iteration_list , [ fit_fcn ( x ) for x in particle . position_list ], '-o' ) velocity_axes [ 0 ] . plot ( iteration_list , particle . velocity_list , '-o' ) personal_best_axes [ 0 ] . plot ( iteration_list [: - 1 ], particle . best_position_list , '-o' ) personal_best_axes [ 1 ] . plot ( iteration_list [: - 1 ], [ fit_fcn ( x ) for x in particle . best_position_list ], '-o' ) position_axes [ 2 ] . boxplot ([[ p . position_list [ i ] for p in particles ] for i in iteration_list ], positions = iteration_list ) position_axes [ 3 ] . boxplot ([[ fit_fcn ( p . position_list [ i ]) for p in particles ] for i in iteration_list ], positions = iteration_list ) velocity_axes [ 1 ] . boxplot ([[ p . velocity_list [ i ] for p in particles ] for i in iteration_list ], positions = iteration_list ) personal_best_axes [ 2 ] . boxplot ([[ p . best_position_list [ i ] for p in particles ] for i in iteration_list [: - 1 ]], positions = iteration_list [: - 1 ]) personal_best_axes [ 3 ] . boxplot ([[ fit_fcn ( p . best_position_list [ i ]) for p in particles ] for i in iteration_list [: - 1 ]], positions = iteration_list [: - 1 ]) global_best_axes [ 0 ] . plot ( iteration_list [: - 1 ], global_best_position_list , '-o' ) global_best_axes [ 1 ] . plot ( iteration_list [: - 1 ], [ fit_fcn ( x ) for x in global_best_position_list ], '-o' ) Exercise Multiply the velocity memory, \\(v_i(t)\\) , with a value between 0 and 1, let's say 0.5. How does the process change? This is the effect of inertia weight. Reduce the value of \\(\\alpha_1\\) to 0.05 while maintaining \\(\\alpha_2\\) at 0.1 and investigate the effect. Reduce the value of \\(\\alpha_1\\) to 0. How does this affect the result? Modify such that \\(\\alpha_1\\) is larger than \\(\\alpha_2\\) . What's the effect? How may you modify the formulae for particles with two variables, in which the fitness function is defined as \\(f(x,y) = x^2 + y^2\\) ? Report Submit a report detailing the process you have gone through and the observations you have made.","title":"Lab 4: Particle Swarm Optimisastion"},{"location":"lab4-particle-swarm-optimisation/#lab-4-particle-swarm-optimisastion","text":"","title":"Lab 4: Particle Swarm Optimisastion"},{"location":"lab4-particle-swarm-optimisation/#lab-learning-outcomes","text":"After completing this lab, the students are able to develop a Python function to perform global best particle swarm optimisation.","title":"Lab learning outcomes"},{"location":"lab4-particle-swarm-optimisation/#setup-for-spyder","text":"If you are using Spyder for this lab, go to Tools > Preferences > IPython console > Graphics and set Backend to Automatic . Restart kernel by going to Consoles > Restart kernel .","title":"Setup for Spyder"},{"location":"lab4-particle-swarm-optimisation/#problem-to-solve","text":"Solve the following problem using global best particle swarm optimisation: Problem Find the value of x to minimise the function \\(f(x) = (x+100)(x+50)(x)(x-20)(x-60)(x-100)\\) for \\(-100 < x < 100\\)","title":"Problem to solve"},{"location":"lab4-particle-swarm-optimisation/#particle-swarm-optimisation","text":"particles initialisation personal best identification global best identification velocity calculation position update repeat from personal best identification until termination","title":"Particle swarm optimisation"},{"location":"lab4-particle-swarm-optimisation/#parameter-definition","text":"With global best particle swarm optimisaton, the position update function is given by \\[x_i(t+1) = x_i(t) + v_i(t+1)\\] and the velocity update function is \\[v_i(t+1) = v_i(t) + \\alpha_1\\beta_1(t) \\Big( p_i(t) - x_i(t) \\Big) + \\alpha_2\\beta_2(t)\\Big(p_g(t) - x_i(t)\\Big)\\] \u03b1 1 and \u03b1 2 are acceleration constants that are fixed throughout the algorithm. Define a small value for \u03b1 1 and \u03b1 2 , for example 0.1 . alpha = [ 0.1 , 0.1 ] \u03b2 1 (t) and \u03b2 2 (t) are random values between 0 and 1 that are regenerated every iteration. Therefore no definition is required. Also, define the number of particles to run the algorithm with. n_particle = 10 Place the definition of these variables in the __main__ block. if __name__ == '__main__' : alpha = [ 0.1 , 0.1 ] n_particle = 10","title":"Parameter definition"},{"location":"lab4-particle-swarm-optimisation/#create-a-class-for-particle","text":"As each particle is an individual, create a Particle class to hold the data of the particle's current position, velocity, and personal best position. class Particle : def __init__ ( self , position = 0 , velocity = 0 ): self . position = position self . velocity = velocity self . best_position = position","title":"Create a class for particle"},{"location":"lab4-particle-swarm-optimisation/#fitness-function","text":"Fitness function is how we can compare different particles. As our goal is to minimise f(x) as stated in the beginning , we will use f(x) as our fitness function. By using f(x) in minimisation problem, it implies that the lower the value of f(x), the better the particle it is. The value of x is the position of the particle. Define the fitness function as a Python function. def fit_fcn ( position ): ... return fitness","title":"Fitness function"},{"location":"lab4-particle-swarm-optimisation/#initialise-particles","text":"Particles are initialised with random positions within the constraints. At initialisation, we may assume that the initial velocities of all the particles. It is possible to initialise particles with non-zero velocities. For now, we will stick to zero initial velocities. Define a Python function that takes the input of the number of particles and the limits of the positions to initialise and return a list of objects of class Particle . Each particle has random position within the limits and zero velocity. def initialise_particles ( n_ptc , position_limits ): # position_limits is a list of two values. The first value is the lower boundary and the second value is the upper boundary. ... return particles Remember to test your function before proceed.","title":"Initialise particles"},{"location":"lab4-particle-swarm-optimisation/#update-personal-best","text":"Create a method in the class Particle to update the best_position if necessary. class Particle : def __init__ ( ... ): ... def update_personal_best ( self ): # 1. calculate the fitnesses of the best_position and the particle's current position # 2. compare the fitnesses and determine if the current position is better than the best_position # 3. update if necessary # 4. no return statement is required If the new position has a lower fitness, i.e. the new position is better than the best position, update the best_position to hold the value of the new position.","title":"Update personal best"},{"location":"lab4-particle-swarm-optimisation/#update-global-best","text":"Initiate a variable named global_best_position with the value None in the __main__ block. Create a function that takes two positions as inputs, compare them, and return the better position of the two. def compareFitness ( pos1 , pos2 ): # 1. calculate the fitness of pos1 and pos2 # 2. compare to determine the better position return betterpos We will later use this function to compare the current global best position with the personal best position of each particle.","title":"Update global best"},{"location":"lab4-particle-swarm-optimisation/#update-velocity","text":"Create a method in the class Particle to update the velocity given \u03b1 1 , \u03b1 2 , \u03b2 1 , \u03b2 2 , and the global best position. class Particle : def __init__ ( ... ): ... def update_personal_best ( ... ): ... def update_velocity ( self , alpha , beta , glob_best_pos ): # alpha is a list of two values. we will access alpha_1 and alpha_2 by alpha[0] and alpha[1] respectively. This also applies to beta. # the current position, current velocity, and personal best position of the particle can be accessed by self.position, self.velocity, and self.best_position # assign the particle's velocity with the updated velocity","title":"Update velocity"},{"location":"lab4-particle-swarm-optimisation/#update-particle-position","text":"As updating a particle position only require information from within the particle object and the limits of the position, create a method called update_position in the class Particle taking the input of the limits of the position. class Particle : def __init__ ( ... ): ... def update_personal_best ( ... ): ... def update_velocity ( ... ): ... def update_position ( self , position_limits ): self . position = self . position + self . velocity # how should you solve the problem of the position (x) going out of the limits","title":"Update particle position"},{"location":"lab4-particle-swarm-optimisation/#create-a-loop-until-termination","text":"Consider the following termination criteria: exceeding 200 iterations fitnesses of all particles are close positions of all particles are close Create a function to calculate the average difference between the mean fitness and the fitness of each particle. def calc_avg_fit_diff ( particles ): # 1. calculate mean fitness of all particles # 2. calculate the difference between the mean fitness and the fitness of each particle # 3. calculate the average of the differences obtained from step 2 return avg_fit_diff Create a function to calculate the average difference between the mean position and the position of each particle. def calc_avg_pos_diff ( particles ): # 1. calculate mean position of all particles # 2. calculate the difference between the mean position and the position of each particle # 3. calculate the average of the differences obtained from step 2 return avg_pos_diff Create a loop (in the __main__ block) to execute the global best particle swarm optimisation (gbest PSO) until termination. if __name__ == '__main__' : # parameter initialisation alpha = [ 0.1 , 0.1 ] n_particle = 10 global_best_position = None position_limits = [ - 100 , 100 ] # termination threshold iteration = 0 max_iter = 200 min_avg_fit_diff = 0.1 min_avg_pos_diff = 0.1 # initialise particles particles = initialise_particles ( n_particle , position_limits ) while ( ... ): # how should you define the termination criteria here? print ( iteration , [ round ( x . position , 2 ) for x in particles ]) for particle in particles : # update personal best particle . update_personal_best () # update global best if global_best_position == None : global_best_position = particle . position else : global_best_position = compareFitness ( global_best_position , particle . position ) # generate beta randomly for current iteration beta = [ random . random (), random . random ()] for particle in particles : # update velocity particle . update_velocity ( alpha , beta , global_best_position ) # update position particle . update_position ( position_limits ) iteration += 1 # display results print ( iteration , [ round ( x . position , 2 ) for x in particles ])","title":"Create a loop (until termination)"},{"location":"lab4-particle-swarm-optimisation/#visualisation","text":"Let's add a few lines to visualise particles \"flying\" towards to optimal position. import the visualisation library import matplotlib.pyplot as plt add the following lines just before the while loop in the last code block in the previous section . This is to plot the existing particle positions on the graph. space_ax = plt . axes () space_ax . plot ( list ( range ( * position_limits )),[ fit_fcn ( x ) for x in range ( * position_limits )]) space_ax . set_title ( \"Position of particles in iteration {} \" . format ( iteration )) space_ax . set_xlabel ( \"Position\" ) space_ax . set_ylabel ( \"Fitness\" ) add the following lines between line 14 and line 15 in the last code block in the previous section , as well as after line 33. This is to remove the existing particle positions and plot the new positions, i.e to visually update the positions. if len ( space_ax . lines ) > 1 : space_ax . lines [ 1 ] . remove () space_ax . plot ([ x . position for x in particles ], [ fit_fcn ( x . position ) for x in particles ], 'go' ) space_ax . set_title ( \"Position of particles in iteration {} \" . format ( iteration )) plt . pause ( 0.5 ) # pause the program for 0.5 second; if graph changes too quickly, increase this value; you can also speed up the process by decreasing this value","title":"Visualisation"},{"location":"lab4-particle-swarm-optimisation/#evaluation","text":"Store the values of the variables at each iteration for analysis and evaluation. position of each particle at each iteration (add the new line of code to the end of the methods) class Particle : def __init__ ( ... ): ... self . position_list = [ position ] def update_position ( ... ): ... self . position_list . append ( self . position ) velocity of each particle at each iteration (add the new line of code to the end of the methods) class Particle : def __init__ ( ... ): ... self . velocity_list = [ velocity ] def update_velocity ( ... ): ... self . velocity_list . append ( self . velocity ) personal best position of each particle at each iteration (add the new line of code to the end of the methods) class Particle : def __init__ ( ... ): ... self . best_position_list = [] def update_personal_best ( ... ): ... self . best_position_list . append ( self . best_position ) global best position at each iteration if __init__ == '__main__' : # parameter initialisation ... global_best_position_list = [] ... global_best_position = ... global_best_position_list . append ( global_best_position ) # take note on the indentation # generate beta randomly for current iteration ... Visualise the progression of these variables by adding the following code to the end of the __main__ block. [ pos_fig , position_axes ] = plt . subplots ( 4 , 1 , sharex = True ) position_axes [ 0 ] . set_title ( \"Position of each particle\" ) position_axes [ 1 ] . set_title ( \"Fitness of each particle\" ) position_axes [ 2 ] . set_title ( \"Boxplot of position at each iteration\" ) position_axes [ 3 ] . set_title ( \"Boxplot of fitness at each iteration\" ) position_axes [ 3 ] . set_xlabel ( \"Iteration\" ) [ vel_fig , velocity_axes ] = plt . subplots ( 2 , 1 , sharex = True ) velocity_axes [ 0 ] . set_title ( \"Velocity of each particle\" ) velocity_axes [ 1 ] . set_title ( \"Boxplot for velocity at each iteration\" ) velocity_axes [ 1 ] . set_xlabel ( \"Iteration\" ) [ p_best_fig , personal_best_axes ] = plt . subplots ( 4 , 1 , sharex = True ) personal_best_axes [ 0 ] . set_title ( \"Personal best position of each particle\" ) personal_best_axes [ 1 ] . set_title ( \"Personal best fitness of each particle\" ) personal_best_axes [ 2 ] . set_title ( \"Boxplot of personal best position at each iteration\" ) personal_best_axes [ 3 ] . set_title ( \"Boxplot of personal best fitness at each iteration\" ) personal_best_axes [ 3 ] . set_xlabel ( \"Iteration\" ) [ g_best_fig , global_best_axes ] = plt . subplots ( 2 , 1 , sharex = True ) global_best_axes [ 0 ] . set_title ( \"Global best position\" ) global_best_axes [ 1 ] . set_title ( \"Fitness for global best position\" ) global_best_axes [ 1 ] . set_xlabel ( \"Iteration\" ) for particle in particles : iteration_list = list ( range ( len ( particle . position_list ))) position_axes [ 0 ] . plot ( iteration_list , particle . position_list , '-o' ) position_axes [ 1 ] . plot ( iteration_list , [ fit_fcn ( x ) for x in particle . position_list ], '-o' ) velocity_axes [ 0 ] . plot ( iteration_list , particle . velocity_list , '-o' ) personal_best_axes [ 0 ] . plot ( iteration_list [: - 1 ], particle . best_position_list , '-o' ) personal_best_axes [ 1 ] . plot ( iteration_list [: - 1 ], [ fit_fcn ( x ) for x in particle . best_position_list ], '-o' ) position_axes [ 2 ] . boxplot ([[ p . position_list [ i ] for p in particles ] for i in iteration_list ], positions = iteration_list ) position_axes [ 3 ] . boxplot ([[ fit_fcn ( p . position_list [ i ]) for p in particles ] for i in iteration_list ], positions = iteration_list ) velocity_axes [ 1 ] . boxplot ([[ p . velocity_list [ i ] for p in particles ] for i in iteration_list ], positions = iteration_list ) personal_best_axes [ 2 ] . boxplot ([[ p . best_position_list [ i ] for p in particles ] for i in iteration_list [: - 1 ]], positions = iteration_list [: - 1 ]) personal_best_axes [ 3 ] . boxplot ([[ fit_fcn ( p . best_position_list [ i ]) for p in particles ] for i in iteration_list [: - 1 ]], positions = iteration_list [: - 1 ]) global_best_axes [ 0 ] . plot ( iteration_list [: - 1 ], global_best_position_list , '-o' ) global_best_axes [ 1 ] . plot ( iteration_list [: - 1 ], [ fit_fcn ( x ) for x in global_best_position_list ], '-o' )","title":"Evaluation"},{"location":"lab4-particle-swarm-optimisation/#exercise","text":"Multiply the velocity memory, \\(v_i(t)\\) , with a value between 0 and 1, let's say 0.5. How does the process change? This is the effect of inertia weight. Reduce the value of \\(\\alpha_1\\) to 0.05 while maintaining \\(\\alpha_2\\) at 0.1 and investigate the effect. Reduce the value of \\(\\alpha_1\\) to 0. How does this affect the result? Modify such that \\(\\alpha_1\\) is larger than \\(\\alpha_2\\) . What's the effect? How may you modify the formulae for particles with two variables, in which the fitness function is defined as \\(f(x,y) = x^2 + y^2\\) ?","title":"Exercise"},{"location":"lab4-particle-swarm-optimisation/#report","text":"Submit a report detailing the process you have gone through and the observations you have made.","title":"Report"},{"location":"lab5-decision-tree/","text":"Lab 5: Decision Tree Lab learning outcomes After completing this lab, the students are able to construct decision tree using CART learning algorithm with the scikit-learn Python library. Datasets The iris dataset will be used for classification, and the diabetes dataset for regression. from sklearn import datasets import pandas as pd iris = datasets . load_iris () iris = { 'attributes' : pd . DataFrame ( iris . data , columns = iris . feature_names ), 'target' : pd . DataFrame ( iris . target , columns = [ 'species' ]), 'targetNames' : iris . target_names } diabetes = datasets . load_diabetes () diabetes = { 'attributes' : pd . DataFrame ( diabetes . data , columns = diabetes . feature_names ), 'target' : pd . DataFrame ( diabetes . target , columns = [ 'diseaseProgression' ]) } Split the datasets into 80-20 for train-test proportion. from sklearn.model_selection import train_test_split for dt in [ iris , diabetes ]: x_train , x_test , y_train , y_test = train_test_split ( dt [ 'attributes' ], dt [ 'target' ], test_size = 0.2 , random_state = 1 ) dt [ 'train' ] = { 'attributes' : x_train , 'target' : y_train } dt [ 'test' ] = { 'attributes' : x_test , 'target' : y_test } Note : Be reminded that random_state is used to reproduce the same \"random\" split of the data whenever the function is called. To produce randomly splitted data every time the function is called, remove the random_state argument. How do we access the training input data for the iris dataset? Decision tree Decision tree models and algorithms are provided by the scikit-learn as the class sklearn.tree.DecisionTreeClassifier for classification, and sklearn.tree.DecisionTreeRegressor for regression. Classification Import the class for decision tree classifier. from sklearn.tree import DecisionTreeClassifier Instantiate an object of DecisionTreeClassifier class with gini impurity as the split criterion. dtc = DecisionTreeClassifier ( criterion = 'gini' ) Train the classifier with the training data. We will use all the input attributes. dtc . fit ( iris [ 'train' ][ 'attributes' ], iris [ 'train' ][ 'target' ]) .predict function is used to predict the species of the testing data. predicts = dtc . predict ( iris [ 'test' ][ 'attributes' ]) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( iris [ 'test' ][ 'target' ] . species , predicts )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. accuracy = dtc . score ( iris [ 'test' ][ 'attributes' ], iris [ 'test' ][ 'target' ] . species ) print ( f 'Accuracy: { accuracy : .4f } ' ) Decision tree visualisation Import the matplotlib.pyplot library and the function to visualise the tree. import matplotlib.pyplot as plt from sklearn.tree import plot_tree Visualise the decision tree model. plt . figure ( figsize = [ 10 , 10 ]) tree = plot_tree ( dtc , feature_names = iris [ 'attributes' ] . columns . tolist (), class_names = list ( iris [ 'targetNames' ]), filled = True , rounded = True ) The maximum depth of a decision tree can be defined by adding the max_depth=... argument to the DecisionTreeClassifier(...) object instantiation. To allow unlimited maximum depth, pass max_depth=None . Create a loop to compare the accuracy of the prediction with different maximum depths. In every iteration, you should calculate both the accuracy on the training data and the accuracy on the testing data. The comparison should be shown in a graph with max_depth as the horizontal axis and accuracy as the vertical axis. Two lines should be displayed on the graph with one line for training accuracy and the other testing accuracy. Visualisation of decision surface This section explains the method to visualise a decision tree on a graph. To do so we will focus on using two input attributes, sepal length and sepal width, i.e. the first two columns. Instantiate the classifier without defining the maximum depth and train the model. dtc = DecisionTreeClassifier () input_cols = iris [ 'train' ][ 'attributes' ] . columns [: 2 ] . tolist () dtc . fit ( iris [ 'train' ][ 'attributes' ][ input_cols ], iris [ 'train' ][ 'target' ] . species ) Plot the decision tree. plt . figure ( figsize = [ 50 , 50 ]) plot_tree ( dtc , feature_names = input_cols , class_names = list ( iris [ 'targetNames' ]), filled = True , rounded = True ) plt . savefig ( 'classificationDecisionTreeWithNoMaxDepth.png' ) Prepare the colormaps. from matplotlib import cm from matplotlib.colors import ListedColormap colormap = cm . get_cmap ( 'tab20' ) cm_dark = ListedColormap ( colormap . colors [:: 2 ]) cm_light = ListedColormap ( colormap . colors [ 1 :: 2 ]) Calculating the decision surface. import numpy as np x_min = iris [ 'attributes' ][ input_cols [ 0 ]] . min () x_max = iris [ 'attributes' ][ input_cols [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = iris [ 'attributes' ][ input_cols [ 1 ]] . min () y_max = iris [ 'attributes' ][ input_cols [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = dtc . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision surface. plt . figure () plt . pcolormesh ( xx , yy , z , cmap = cm_light ) Plot the training and testing data. plt . scatter ( iris [ 'train' ][ 'attributes' ][ input_cols [ 0 ]], iris [ 'train' ][ 'attributes' ][ input_cols [ 1 ]], c = iris [ 'train' ][ 'target' ] . species , cmap = cm_dark , s = 200 , label = 'Training data' , edgecolor = 'black' , linewidth = 1 ) plt . scatter ( iris [ 'test' ][ 'attributes' ][ input_cols [ 0 ]], iris [ 'test' ][ 'attributes' ][ input_cols [ 1 ]], c = iris [ 'test' ][ 'target' ] . species , cmap = cm_dark , s = 200 , label = 'Testing data' , edgecolor = 'black' , linewidth = 1 , marker = '*' ) train_acc = dtc . score ( iris [ 'train' ][ 'attributes' ][ input_cols ], iris [ 'train' ][ 'target' ] . species ) test_acc = dtc . score ( iris [ 'test' ][ 'attributes' ][ input_cols ], iris [ 'test' ][ 'target' ] . species ) plt . title ( f 'training: { train_acc : .3f } , testing: { test_acc : .3f } ' ) plt . xlabel ( input_cols [ 0 ]) plt . ylabel ( input_cols [ 1 ]) plt . legend () You may not be able to see anything on one of the graph of the decision tree because the figure size is set to be larger than the screen size. However, the tree is saved to a png file in the same folder as your code. Overfitting Now, train a decision tree classifier of max_depth=3 with the two input attributes used in the previous section, sepal length and sepal width. Plot the decision surface for this classifier after the training. Compare the decision surface, training accuracy, and testing accuracy between this model and the model in the previous section. Discuss the comparison between the decision tree from previous section and that of max_depth=3 from the aspect of overfitting/generalisation. Regression Regression using decision tree can be achieved by using the DecisionTreeRegressor class in sklearn.tree . Instantiate a regressor class and train the regressor with the training data using all the input attributes. Predict the disease progression of the testing data, and determine the accuracy of the prediction. Create a plot of prediction accuracies against maximum depths of the decision tree for both training data and testing data. Visualisation of decision surface This section explains the method to visualise a decision tree on a graph. To do so we will focus on using two input attributes, age and bmi . Instantiate the classifier without defining the maximum depth and train the model. dtr = DecisiontTreeRegressor () input_cols = [ 'age' , 'bmi' ] dtr . fit ( diabetes [ 'train' ][ 'attributes' ][ input_cols ], diabetes [ 'train' ][ 'target' ] . diseaseProgression ) Plot the decision tree. plt . figure ( figsize = [ 50 , 50 ]) plot_tree ( dtr , feature_names = input_cols , filled = True , rounded = True ) plt . savefig ( 'regressionDecisionTreeWithNoMaxDepth.png' ) Prepare the colormaps. from matplotlib import cm dia_cm = cm . get_cmap ( 'Reds' ) Create the decision surface. import numpy as np x_min = diabetes [ 'attributes' ][ input_cols [ 0 ]] . min () x_max = diabetes [ 'attributes' ][ input_cols [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = diabetes [ 'attributes' ][ input_cols [ 1 ]] . min () y_max = diabetes [ 'attributes' ][ input_cols [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = dtr . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision surface plt . figure () plt . pcolormesh ( xx , yy , z , cmap = dia_cm ) Plot the training and testing data. plt . scatter ( diabetes [ 'train' ][ 'attributes' ][ input_cols [ 0 ]], diabetes [ 'train' ][ 'attributes' ][ input_cols [ 1 ]], c = diabetes [ 'train' ][ 'target' ] . diseaseProgression , label = 'Training data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( diabetes [ 'test' ][ 'attributes' ][ input_cols [ 0 ]], diabetes [ 'test' ][ 'attributes' ][ input_cols [ 1 ]], c = diabetes [ 'test' ][ 'target' ] . diseaseProgression , marker = '*' , label = 'Testing data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_cols [ 0 ]) plt . ylabel ( input_cols [ 1 ]) plt . legend () plt . colorbar () Overfitting Compare the decision tree regressor in the previous model with a decision tree regressor of a small maximum depth and discuss overfitting using the decision surface, training accuracy, and testing accuracy. Report Submit a written report that describes the considerations while writing the scripts, the answers to the above questions, and the problems you encountered in the process. Include your script in the written report as an appendix.","title":"Lab 5: Decision Tree"},{"location":"lab5-decision-tree/#lab-5-decision-tree","text":"","title":"Lab 5: Decision Tree"},{"location":"lab5-decision-tree/#lab-learning-outcomes","text":"After completing this lab, the students are able to construct decision tree using CART learning algorithm with the scikit-learn Python library.","title":"Lab learning outcomes"},{"location":"lab5-decision-tree/#datasets","text":"The iris dataset will be used for classification, and the diabetes dataset for regression. from sklearn import datasets import pandas as pd iris = datasets . load_iris () iris = { 'attributes' : pd . DataFrame ( iris . data , columns = iris . feature_names ), 'target' : pd . DataFrame ( iris . target , columns = [ 'species' ]), 'targetNames' : iris . target_names } diabetes = datasets . load_diabetes () diabetes = { 'attributes' : pd . DataFrame ( diabetes . data , columns = diabetes . feature_names ), 'target' : pd . DataFrame ( diabetes . target , columns = [ 'diseaseProgression' ]) } Split the datasets into 80-20 for train-test proportion. from sklearn.model_selection import train_test_split for dt in [ iris , diabetes ]: x_train , x_test , y_train , y_test = train_test_split ( dt [ 'attributes' ], dt [ 'target' ], test_size = 0.2 , random_state = 1 ) dt [ 'train' ] = { 'attributes' : x_train , 'target' : y_train } dt [ 'test' ] = { 'attributes' : x_test , 'target' : y_test } Note : Be reminded that random_state is used to reproduce the same \"random\" split of the data whenever the function is called. To produce randomly splitted data every time the function is called, remove the random_state argument. How do we access the training input data for the iris dataset?","title":"Datasets"},{"location":"lab5-decision-tree/#decision-tree","text":"Decision tree models and algorithms are provided by the scikit-learn as the class sklearn.tree.DecisionTreeClassifier for classification, and sklearn.tree.DecisionTreeRegressor for regression.","title":"Decision tree"},{"location":"lab5-decision-tree/#classification","text":"Import the class for decision tree classifier. from sklearn.tree import DecisionTreeClassifier Instantiate an object of DecisionTreeClassifier class with gini impurity as the split criterion. dtc = DecisionTreeClassifier ( criterion = 'gini' ) Train the classifier with the training data. We will use all the input attributes. dtc . fit ( iris [ 'train' ][ 'attributes' ], iris [ 'train' ][ 'target' ]) .predict function is used to predict the species of the testing data. predicts = dtc . predict ( iris [ 'test' ][ 'attributes' ]) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( iris [ 'test' ][ 'target' ] . species , predicts )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. accuracy = dtc . score ( iris [ 'test' ][ 'attributes' ], iris [ 'test' ][ 'target' ] . species ) print ( f 'Accuracy: { accuracy : .4f } ' ) Decision tree visualisation Import the matplotlib.pyplot library and the function to visualise the tree. import matplotlib.pyplot as plt from sklearn.tree import plot_tree Visualise the decision tree model. plt . figure ( figsize = [ 10 , 10 ]) tree = plot_tree ( dtc , feature_names = iris [ 'attributes' ] . columns . tolist (), class_names = list ( iris [ 'targetNames' ]), filled = True , rounded = True ) The maximum depth of a decision tree can be defined by adding the max_depth=... argument to the DecisionTreeClassifier(...) object instantiation. To allow unlimited maximum depth, pass max_depth=None . Create a loop to compare the accuracy of the prediction with different maximum depths. In every iteration, you should calculate both the accuracy on the training data and the accuracy on the testing data. The comparison should be shown in a graph with max_depth as the horizontal axis and accuracy as the vertical axis. Two lines should be displayed on the graph with one line for training accuracy and the other testing accuracy.","title":"Classification"},{"location":"lab5-decision-tree/#visualisation-of-decision-surface","text":"This section explains the method to visualise a decision tree on a graph. To do so we will focus on using two input attributes, sepal length and sepal width, i.e. the first two columns. Instantiate the classifier without defining the maximum depth and train the model. dtc = DecisionTreeClassifier () input_cols = iris [ 'train' ][ 'attributes' ] . columns [: 2 ] . tolist () dtc . fit ( iris [ 'train' ][ 'attributes' ][ input_cols ], iris [ 'train' ][ 'target' ] . species ) Plot the decision tree. plt . figure ( figsize = [ 50 , 50 ]) plot_tree ( dtc , feature_names = input_cols , class_names = list ( iris [ 'targetNames' ]), filled = True , rounded = True ) plt . savefig ( 'classificationDecisionTreeWithNoMaxDepth.png' ) Prepare the colormaps. from matplotlib import cm from matplotlib.colors import ListedColormap colormap = cm . get_cmap ( 'tab20' ) cm_dark = ListedColormap ( colormap . colors [:: 2 ]) cm_light = ListedColormap ( colormap . colors [ 1 :: 2 ]) Calculating the decision surface. import numpy as np x_min = iris [ 'attributes' ][ input_cols [ 0 ]] . min () x_max = iris [ 'attributes' ][ input_cols [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = iris [ 'attributes' ][ input_cols [ 1 ]] . min () y_max = iris [ 'attributes' ][ input_cols [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = dtc . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision surface. plt . figure () plt . pcolormesh ( xx , yy , z , cmap = cm_light ) Plot the training and testing data. plt . scatter ( iris [ 'train' ][ 'attributes' ][ input_cols [ 0 ]], iris [ 'train' ][ 'attributes' ][ input_cols [ 1 ]], c = iris [ 'train' ][ 'target' ] . species , cmap = cm_dark , s = 200 , label = 'Training data' , edgecolor = 'black' , linewidth = 1 ) plt . scatter ( iris [ 'test' ][ 'attributes' ][ input_cols [ 0 ]], iris [ 'test' ][ 'attributes' ][ input_cols [ 1 ]], c = iris [ 'test' ][ 'target' ] . species , cmap = cm_dark , s = 200 , label = 'Testing data' , edgecolor = 'black' , linewidth = 1 , marker = '*' ) train_acc = dtc . score ( iris [ 'train' ][ 'attributes' ][ input_cols ], iris [ 'train' ][ 'target' ] . species ) test_acc = dtc . score ( iris [ 'test' ][ 'attributes' ][ input_cols ], iris [ 'test' ][ 'target' ] . species ) plt . title ( f 'training: { train_acc : .3f } , testing: { test_acc : .3f } ' ) plt . xlabel ( input_cols [ 0 ]) plt . ylabel ( input_cols [ 1 ]) plt . legend () You may not be able to see anything on one of the graph of the decision tree because the figure size is set to be larger than the screen size. However, the tree is saved to a png file in the same folder as your code.","title":"Visualisation of decision surface"},{"location":"lab5-decision-tree/#overfitting","text":"Now, train a decision tree classifier of max_depth=3 with the two input attributes used in the previous section, sepal length and sepal width. Plot the decision surface for this classifier after the training. Compare the decision surface, training accuracy, and testing accuracy between this model and the model in the previous section. Discuss the comparison between the decision tree from previous section and that of max_depth=3 from the aspect of overfitting/generalisation.","title":"Overfitting"},{"location":"lab5-decision-tree/#regression","text":"Regression using decision tree can be achieved by using the DecisionTreeRegressor class in sklearn.tree . Instantiate a regressor class and train the regressor with the training data using all the input attributes. Predict the disease progression of the testing data, and determine the accuracy of the prediction. Create a plot of prediction accuracies against maximum depths of the decision tree for both training data and testing data.","title":"Regression"},{"location":"lab5-decision-tree/#visualisation-of-decision-surface_1","text":"This section explains the method to visualise a decision tree on a graph. To do so we will focus on using two input attributes, age and bmi . Instantiate the classifier without defining the maximum depth and train the model. dtr = DecisiontTreeRegressor () input_cols = [ 'age' , 'bmi' ] dtr . fit ( diabetes [ 'train' ][ 'attributes' ][ input_cols ], diabetes [ 'train' ][ 'target' ] . diseaseProgression ) Plot the decision tree. plt . figure ( figsize = [ 50 , 50 ]) plot_tree ( dtr , feature_names = input_cols , filled = True , rounded = True ) plt . savefig ( 'regressionDecisionTreeWithNoMaxDepth.png' ) Prepare the colormaps. from matplotlib import cm dia_cm = cm . get_cmap ( 'Reds' ) Create the decision surface. import numpy as np x_min = diabetes [ 'attributes' ][ input_cols [ 0 ]] . min () x_max = diabetes [ 'attributes' ][ input_cols [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = diabetes [ 'attributes' ][ input_cols [ 1 ]] . min () y_max = diabetes [ 'attributes' ][ input_cols [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = dtr . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision surface plt . figure () plt . pcolormesh ( xx , yy , z , cmap = dia_cm ) Plot the training and testing data. plt . scatter ( diabetes [ 'train' ][ 'attributes' ][ input_cols [ 0 ]], diabetes [ 'train' ][ 'attributes' ][ input_cols [ 1 ]], c = diabetes [ 'train' ][ 'target' ] . diseaseProgression , label = 'Training data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( diabetes [ 'test' ][ 'attributes' ][ input_cols [ 0 ]], diabetes [ 'test' ][ 'attributes' ][ input_cols [ 1 ]], c = diabetes [ 'test' ][ 'target' ] . diseaseProgression , marker = '*' , label = 'Testing data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_cols [ 0 ]) plt . ylabel ( input_cols [ 1 ]) plt . legend () plt . colorbar ()","title":"Visualisation of decision surface"},{"location":"lab5-decision-tree/#overfitting_1","text":"Compare the decision tree regressor in the previous model with a decision tree regressor of a small maximum depth and discuss overfitting using the decision surface, training accuracy, and testing accuracy.","title":"Overfitting"},{"location":"lab5-decision-tree/#report","text":"Submit a written report that describes the considerations while writing the scripts, the answers to the above questions, and the problems you encountered in the process. Include your script in the written report as an appendix.","title":"Report"},{"location":"archive/202301/get-start/","text":"Getting started The labs for ETC2073 Artificial Intelligence will be using Python as the programming language. The Anaconda distribution of Python is recommended, however, if you are familiar and comfortable with the vanilla distribution of Python. For those who have not installed Python in their machine, proceed to next session . This page only covers the installation of the Anaconda platform as, in my opinion, it is more beginner friendly in terms of installing packages and encapsulating environments. Installation Download the Anaconda installer with Python 3 for your system from https://www.anaconda.com/distribution/ . Use the graphical installer to install Anaconda. Launching IDE You will be using the Spyder IDE for Python. Feel free to use other IDE or code editor with terminal if that's your preference. Start the Anaconda Navigator from your application list. From Anaconda base environment, launch Spyder IDE. The Spyder IDE consists of three parts: the editor, the variable explorer, and the IPython Console. The editor is where you write your codes. The variable explorer shows the value of the variable after running the code. The IPython console allows you to execute commands, interact with the running code, and display visualisation. After the code is written in the editor, you can execute the code using F5 to run file. Then the variables and their values can be found in the variable explorer.","title":"Getting started"},{"location":"archive/202301/get-start/#getting-started","text":"The labs for ETC2073 Artificial Intelligence will be using Python as the programming language. The Anaconda distribution of Python is recommended, however, if you are familiar and comfortable with the vanilla distribution of Python. For those who have not installed Python in their machine, proceed to next session . This page only covers the installation of the Anaconda platform as, in my opinion, it is more beginner friendly in terms of installing packages and encapsulating environments.","title":"Getting started"},{"location":"archive/202301/get-start/#installation","text":"Download the Anaconda installer with Python 3 for your system from https://www.anaconda.com/distribution/ . Use the graphical installer to install Anaconda.","title":"Installation"},{"location":"archive/202301/get-start/#launching-ide","text":"You will be using the Spyder IDE for Python. Feel free to use other IDE or code editor with terminal if that's your preference. Start the Anaconda Navigator from your application list. From Anaconda base environment, launch Spyder IDE. The Spyder IDE consists of three parts: the editor, the variable explorer, and the IPython Console. The editor is where you write your codes. The variable explorer shows the value of the variable after running the code. The IPython console allows you to execute commands, interact with the running code, and display visualisation. After the code is written in the editor, you can execute the code using F5 to run file. Then the variables and their values can be found in the variable explorer.","title":"Launching IDE"},{"location":"archive/202301/lab1-fuzzy/","text":"Lab 1: Fuzzy Systems Lab learning outcomes After completing this lab, the students are able to construct a Mamdani fuzzy system using the scikit-fuzzy Python library and evaluate the result of the constructed fuzzy system. Note Install the scikit-fuzzy Python library in your environment before proceeding with the lab. conda install - c conda - forge scikit - fuzzy Fuzzy control system for a train Consider a fuzzy control system to control the brake and throttle of a train based on the speed of the train and the distance of the train to the next stop. Import the skfuzzy , skfuzzy.control , and numpy . import numpy as np from skfuzzy import control as ctrl from skfuzzy import membership as mf Initialise inputs and outputs Speed and distance are the inputs of the system whereas brake and throttle are the outputs. The ranges for the variables are: Variable Range Speed 0 - 85 km/h Distance 0 - 3000 m Brake 0 - 100% Throttle 0 - 100% As the inputs will be the antecedents of the rules, construct the variables speed and distance as skfuzzy.control.Antecedent objects. speed = ctrl . Antecedent ( np . arange ( 0 , 85 , 0.1 ), 'speed' ) The initialisation function for skfuzzy.control.Antecedent object takes 2 arguments, the first is the universe of the variable, i.e. the values the variables can take, the second is the label of the variable. The initialisation function for skfuzzy.control.Consequent is similar. The label and the range of the variable can be accessed using .label and .universe respectively. Task : Initialise the variables distance as Antecedent object, and brake and throttle as Consequent objects. (Outputs of the system will be consequents of the rules) Define membership functions for fuzzy sets of variables The fit vectors of the fuzzy sets for the linguistic variables are given as follows: speed (0 to 85 km/h) Linguistic value Fit vector Stopped (1/0, 0/2) Very slow (0/1, 1/2.5, 0/4) Slow (0/2.5, 1/6.5, 0/10.5) Medium fast (0/6.5, 1/26.5, 0/46.5) Fast (0/26.5, 1/70, 1/85) distance (0 to 3000 m) Linguistic value Fit vector At (1/0, 0/2) Very near (0/1, 1/3, 0/5) Near (0/3, 1/101.5, 0/200) Medium far (0/100, 1/1550, 0/3000) Far (0/1500, 1/2250, 1/3000) brake (0 to 100%) Linguistic value Fit vector No (1/0, 0/40) Very slight (0/20, 1/50, 0/80) Slight (0/70, 1/83.5, 0/97) Medium (0/95, 1/97, 0/99) Full (0/98, 1/100) throttle (0 to 100%) Linguistic value Fit vector No (1/0, 0/2) Very slight (0/1, 1/3, 0/5) Slight (0/3, 1/16.5, 0/30) Medium (0/20, 1/50, 0/80) Full (0/60, 1/80, 1/100) The skfuzzy.membership module provides the following membership functions: Membership function Description skfuzzy.membership.dsigmf(x, b1, c1, b2, c2) Difference of two fuzzy sigmoid membership functions skfuzzy.membership.gauss2mf(x, mean1, ...) Gaussian fuzzy membership function of two combined Gaussians skfuzzy.membership.gaussmf(x, mean, sigma) Gaussian fuzzy membership function skfuzzy.membership.gbellmf(x, a, b, c) Generalized Bell function fuzzy membership generator skfuzzy.membership.piecemf(x, abc) Piecewise linear membership function (particularly used in FIRE filters) skfuzzy.membership.pimf(x, a, b, c, d) Pi-function fuzzy membership generator skfuzzy.membership.psigmf(x, b1, c1, b2, c2) Product of two sigmoid membership functions skfuzzy.membership.sigmf(x, b, c) The basic sigmoid membership function generator skfuzzy.membership.smf(x, a, b) S-function fuzzy membership generator skfuzzy.membership.trapmf(x, abcd) Trapezoidal membership function generator skfuzzy.membership.trimf(x, abc) Triangular membership function generator skfuzzy.membership.zmf(x, a, b) Z-function fuzzy membership generator The fit vector of a linguitic value can be assigned to a linguistic variable using speed [ 'stopped' ] = mf . trimf ( speed . universe , [ 0 , 0 , 2 ]) speed [ 'very slow' ] = mf . trimf ( speed . universe , [ 1 , 2.5 , 4 ]) Task : Assign all fuzzy sets to the linguistic variables. The fuzzy set diagram of a linguistic variable can be viewed using .view() speed . view () Task : Check if the fuzzy set diagrams match the fit vectors. Define rules The rules for this system are displayed in the following fuzzy association memory (FAM) representaion table. Distance At Very near Near Medium far Far Speed Stopped Full brake No throttle Full brake Very slight throttle Very slow Full brake No throttle Medium brake Very slight throttle Slight brake Very slight throttle Slow Full brake No throttle Medium brake Very slight throttle Very slight brake Slight throttle Medium fast Very slight brake Medium throttle No brake Full throttle Fast Very slight brake Medium throttle No brake Full throttle Rule can be defined using skfuzzy.control.Rule(antecedent, consequent, label) . To define the first rule, i.e. if distance is 'at' and speed is 'stopped', then full brake and no throttle, rule1 = ctrl . Rule ( distance [ 'at' ] & speed [ 'stopped' ], ( brake [ 'full' ], throttle [ 'no' ])) If the antecedent consists of multiple parts, they can be combined using operators | (OR), & (AND), and ~ (NOT). If the consequent consists of multiple parts, they can be combined as a list / tuple . Task : Define all the rules. Then combine all the rules in a list , i.e. rules = [rule1, rule2, ...] . Construct the fuzzy control system The train control system can be constructed with train_ctrl = ctrl . ControlSystem ( rules = rules ) A skfuzzy.control.ControlSystemSimulation object is needed to simulate the control system to obtain the outputs given certain inputs. train = ctrl . ControlSystemSimulation ( control_system = train_ctrl ) To obtain the values for brake and throttle given that speed is 30 km/h and distance is 6 m, # define the values for the inputs train . input [ 'speed' ] = 30 train . input [ 'distance' ] = 2000 # compute the outputs train . compute () # print the output values print ( train . output ) # to extract one of the outputs print ( train . output [ 'brake' ]) To view the results in the graph, brake . view ( sim = train ) throttle . view ( sim = train ) View the control/output space The control/output space allows us to identify if the outputs fit our expectation. Construct an empty 3D space with 100-by-100 x-y grid. x , y = np . meshgrid ( np . linspace ( speed . universe . min (), speed . universe . max (), 100 ), np . linspace ( distance . universe . min (), distance . universe . max (), 100 )) z_brake = np . zeros_like ( x , dtype = float ) z_throttle = np . zeros_like ( x , dtype = float ) Loop through every point and identify the value of brake and throttle of each point. As the specified rules are not exhaustive, i.e. some input combinations do not activate any rule, we will set the output of such input combinations to be float('inf') . for i , r in enumerate ( x ): for j , c in enumerate ( r ): train . input [ 'speed' ] = x [ i , j ] train . input [ 'distance' ] = y [ i , j ] try : train . compute () except : z_brake [ i , j ] = float ( 'inf' ) z_throttle [ i , j ] = float ( 'inf' ) z_brake [ i , j ] = train . output [ 'brake' ] z_throttle [ i , j ] = train . output [ 'throttle' ] Plot the result in a 3D graph using the matplotlib.pyplot library. import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D def plot3d ( x , y , z ): fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . plot_surface ( x , y , z , rstride = 1 , cstride = 1 , cmap = 'viridis' , linewidth = 0.4 , antialiased = True ) ax . contourf ( x , y , z , zdir = 'z' , offset =- 2.5 , cmap = 'viridis' , alpha = 0.5 ) ax . contourf ( x , y , z , zdir = 'x' , offset = x . max () * 1.5 , cmap = 'viridis' , alpha = 0.5 ) ax . contourf ( x , y , z , zdir = 'y' , offset = y . max () * 1.5 , cmap = 'viridis' , alpha = 0.5 ) ax . view_init ( 30 , 200 ) plot3d ( x , y , z_brake ) plot3d ( x , y , z_throttle ) Fuzzy tipping recommendation system A fuzzy expert system is designed to identify the percentage of tips a customer will give based on the service and the food the customer received. The system has service rating and food rating as inputs, and tips as output. The fit vectors of the fuzzy sets for the linguistic variables are given as follows: service (0 to 10) Linguistic value Fit vector Poor (1/0, 0/5) Average (0/0, 1/5, 0/10) Good (0/5, 1/10) food (0 to 10) Linguistic value Fit vector Poor (1/0, 0/5) Average (0/0, 1/5, 0/10) Good (0/5, 1/10) tips (0 to 30%) Linguistic value Fit vector Low (1/0, 0/15) Medium (0/0, 1/15, 0/30) High (0/15, 1/30) The rules are displayed in the following fuzzy association memory (FAM) representaion table. Food Poor Average Good Service Poor low tips low tips medium tips Average low tips medium tips high tips Good medium tips high tips high tips Task : Construct the fuzzy inference system. Task : Discuss, if you were to make at least one modification to the fuzzy tipping recommendation system, what it will be and why. Report Submit a report detailing the process, results, graphs, and your observations.","title":"Lab 1: Fuzzy Systems"},{"location":"archive/202301/lab1-fuzzy/#lab-1-fuzzy-systems","text":"","title":"Lab 1: Fuzzy Systems"},{"location":"archive/202301/lab1-fuzzy/#lab-learning-outcomes","text":"After completing this lab, the students are able to construct a Mamdani fuzzy system using the scikit-fuzzy Python library and evaluate the result of the constructed fuzzy system.","title":"Lab learning outcomes"},{"location":"archive/202301/lab1-fuzzy/#note","text":"Install the scikit-fuzzy Python library in your environment before proceeding with the lab. conda install - c conda - forge scikit - fuzzy","title":"Note"},{"location":"archive/202301/lab1-fuzzy/#fuzzy-control-system-for-a-train","text":"Consider a fuzzy control system to control the brake and throttle of a train based on the speed of the train and the distance of the train to the next stop. Import the skfuzzy , skfuzzy.control , and numpy . import numpy as np from skfuzzy import control as ctrl from skfuzzy import membership as mf","title":"Fuzzy control system for a train"},{"location":"archive/202301/lab1-fuzzy/#initialise-inputs-and-outputs","text":"Speed and distance are the inputs of the system whereas brake and throttle are the outputs. The ranges for the variables are: Variable Range Speed 0 - 85 km/h Distance 0 - 3000 m Brake 0 - 100% Throttle 0 - 100% As the inputs will be the antecedents of the rules, construct the variables speed and distance as skfuzzy.control.Antecedent objects. speed = ctrl . Antecedent ( np . arange ( 0 , 85 , 0.1 ), 'speed' ) The initialisation function for skfuzzy.control.Antecedent object takes 2 arguments, the first is the universe of the variable, i.e. the values the variables can take, the second is the label of the variable. The initialisation function for skfuzzy.control.Consequent is similar. The label and the range of the variable can be accessed using .label and .universe respectively. Task : Initialise the variables distance as Antecedent object, and brake and throttle as Consequent objects. (Outputs of the system will be consequents of the rules)","title":"Initialise inputs and outputs"},{"location":"archive/202301/lab1-fuzzy/#define-membership-functions-for-fuzzy-sets-of-variables","text":"The fit vectors of the fuzzy sets for the linguistic variables are given as follows: speed (0 to 85 km/h) Linguistic value Fit vector Stopped (1/0, 0/2) Very slow (0/1, 1/2.5, 0/4) Slow (0/2.5, 1/6.5, 0/10.5) Medium fast (0/6.5, 1/26.5, 0/46.5) Fast (0/26.5, 1/70, 1/85) distance (0 to 3000 m) Linguistic value Fit vector At (1/0, 0/2) Very near (0/1, 1/3, 0/5) Near (0/3, 1/101.5, 0/200) Medium far (0/100, 1/1550, 0/3000) Far (0/1500, 1/2250, 1/3000) brake (0 to 100%) Linguistic value Fit vector No (1/0, 0/40) Very slight (0/20, 1/50, 0/80) Slight (0/70, 1/83.5, 0/97) Medium (0/95, 1/97, 0/99) Full (0/98, 1/100) throttle (0 to 100%) Linguistic value Fit vector No (1/0, 0/2) Very slight (0/1, 1/3, 0/5) Slight (0/3, 1/16.5, 0/30) Medium (0/20, 1/50, 0/80) Full (0/60, 1/80, 1/100) The skfuzzy.membership module provides the following membership functions: Membership function Description skfuzzy.membership.dsigmf(x, b1, c1, b2, c2) Difference of two fuzzy sigmoid membership functions skfuzzy.membership.gauss2mf(x, mean1, ...) Gaussian fuzzy membership function of two combined Gaussians skfuzzy.membership.gaussmf(x, mean, sigma) Gaussian fuzzy membership function skfuzzy.membership.gbellmf(x, a, b, c) Generalized Bell function fuzzy membership generator skfuzzy.membership.piecemf(x, abc) Piecewise linear membership function (particularly used in FIRE filters) skfuzzy.membership.pimf(x, a, b, c, d) Pi-function fuzzy membership generator skfuzzy.membership.psigmf(x, b1, c1, b2, c2) Product of two sigmoid membership functions skfuzzy.membership.sigmf(x, b, c) The basic sigmoid membership function generator skfuzzy.membership.smf(x, a, b) S-function fuzzy membership generator skfuzzy.membership.trapmf(x, abcd) Trapezoidal membership function generator skfuzzy.membership.trimf(x, abc) Triangular membership function generator skfuzzy.membership.zmf(x, a, b) Z-function fuzzy membership generator The fit vector of a linguitic value can be assigned to a linguistic variable using speed [ 'stopped' ] = mf . trimf ( speed . universe , [ 0 , 0 , 2 ]) speed [ 'very slow' ] = mf . trimf ( speed . universe , [ 1 , 2.5 , 4 ]) Task : Assign all fuzzy sets to the linguistic variables. The fuzzy set diagram of a linguistic variable can be viewed using .view() speed . view () Task : Check if the fuzzy set diagrams match the fit vectors.","title":"Define membership functions for fuzzy sets of variables"},{"location":"archive/202301/lab1-fuzzy/#define-rules","text":"The rules for this system are displayed in the following fuzzy association memory (FAM) representaion table. Distance At Very near Near Medium far Far Speed Stopped Full brake No throttle Full brake Very slight throttle Very slow Full brake No throttle Medium brake Very slight throttle Slight brake Very slight throttle Slow Full brake No throttle Medium brake Very slight throttle Very slight brake Slight throttle Medium fast Very slight brake Medium throttle No brake Full throttle Fast Very slight brake Medium throttle No brake Full throttle Rule can be defined using skfuzzy.control.Rule(antecedent, consequent, label) . To define the first rule, i.e. if distance is 'at' and speed is 'stopped', then full brake and no throttle, rule1 = ctrl . Rule ( distance [ 'at' ] & speed [ 'stopped' ], ( brake [ 'full' ], throttle [ 'no' ])) If the antecedent consists of multiple parts, they can be combined using operators | (OR), & (AND), and ~ (NOT). If the consequent consists of multiple parts, they can be combined as a list / tuple . Task : Define all the rules. Then combine all the rules in a list , i.e. rules = [rule1, rule2, ...] .","title":"Define rules"},{"location":"archive/202301/lab1-fuzzy/#construct-the-fuzzy-control-system","text":"The train control system can be constructed with train_ctrl = ctrl . ControlSystem ( rules = rules ) A skfuzzy.control.ControlSystemSimulation object is needed to simulate the control system to obtain the outputs given certain inputs. train = ctrl . ControlSystemSimulation ( control_system = train_ctrl ) To obtain the values for brake and throttle given that speed is 30 km/h and distance is 6 m, # define the values for the inputs train . input [ 'speed' ] = 30 train . input [ 'distance' ] = 2000 # compute the outputs train . compute () # print the output values print ( train . output ) # to extract one of the outputs print ( train . output [ 'brake' ]) To view the results in the graph, brake . view ( sim = train ) throttle . view ( sim = train )","title":"Construct the fuzzy control system"},{"location":"archive/202301/lab1-fuzzy/#view-the-controloutput-space","text":"The control/output space allows us to identify if the outputs fit our expectation. Construct an empty 3D space with 100-by-100 x-y grid. x , y = np . meshgrid ( np . linspace ( speed . universe . min (), speed . universe . max (), 100 ), np . linspace ( distance . universe . min (), distance . universe . max (), 100 )) z_brake = np . zeros_like ( x , dtype = float ) z_throttle = np . zeros_like ( x , dtype = float ) Loop through every point and identify the value of brake and throttle of each point. As the specified rules are not exhaustive, i.e. some input combinations do not activate any rule, we will set the output of such input combinations to be float('inf') . for i , r in enumerate ( x ): for j , c in enumerate ( r ): train . input [ 'speed' ] = x [ i , j ] train . input [ 'distance' ] = y [ i , j ] try : train . compute () except : z_brake [ i , j ] = float ( 'inf' ) z_throttle [ i , j ] = float ( 'inf' ) z_brake [ i , j ] = train . output [ 'brake' ] z_throttle [ i , j ] = train . output [ 'throttle' ] Plot the result in a 3D graph using the matplotlib.pyplot library. import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D def plot3d ( x , y , z ): fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . plot_surface ( x , y , z , rstride = 1 , cstride = 1 , cmap = 'viridis' , linewidth = 0.4 , antialiased = True ) ax . contourf ( x , y , z , zdir = 'z' , offset =- 2.5 , cmap = 'viridis' , alpha = 0.5 ) ax . contourf ( x , y , z , zdir = 'x' , offset = x . max () * 1.5 , cmap = 'viridis' , alpha = 0.5 ) ax . contourf ( x , y , z , zdir = 'y' , offset = y . max () * 1.5 , cmap = 'viridis' , alpha = 0.5 ) ax . view_init ( 30 , 200 ) plot3d ( x , y , z_brake ) plot3d ( x , y , z_throttle )","title":"View the control/output space"},{"location":"archive/202301/lab1-fuzzy/#fuzzy-tipping-recommendation-system","text":"A fuzzy expert system is designed to identify the percentage of tips a customer will give based on the service and the food the customer received. The system has service rating and food rating as inputs, and tips as output. The fit vectors of the fuzzy sets for the linguistic variables are given as follows: service (0 to 10) Linguistic value Fit vector Poor (1/0, 0/5) Average (0/0, 1/5, 0/10) Good (0/5, 1/10) food (0 to 10) Linguistic value Fit vector Poor (1/0, 0/5) Average (0/0, 1/5, 0/10) Good (0/5, 1/10) tips (0 to 30%) Linguistic value Fit vector Low (1/0, 0/15) Medium (0/0, 1/15, 0/30) High (0/15, 1/30) The rules are displayed in the following fuzzy association memory (FAM) representaion table. Food Poor Average Good Service Poor low tips low tips medium tips Average low tips medium tips high tips Good medium tips high tips high tips Task : Construct the fuzzy inference system. Task : Discuss, if you were to make at least one modification to the fuzzy tipping recommendation system, what it will be and why.","title":"Fuzzy tipping recommendation system"},{"location":"archive/202301/lab1-fuzzy/#report","text":"Submit a report detailing the process, results, graphs, and your observations.","title":"Report"},{"location":"archive/202301/lab2-uninformed-search/","text":"Lab 2: Breadth-First Search Lab learning outcomes After completing this lab, the students are able to create Python script to execute breadth-first search algorithm to solve a search problem. Nick's route-finding problem in Romania The first search problem we are focusing on is Nick\u2019s route-finding problem in Romania, starting in Arad to reach Bucharest. The road map of Romania, which is the state space of the problem is given as follows: 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\", data: { circleradius: 10 } }); To solve the problem, we need to define the representation of the state, determine the transition model, and the execution of the search algorithm. State representation In this problem the only state we need to consider is the location of Nick. Therefore we can use the names of the cities as the state. Since Nick is starting in Arad, going to Bucharest, we can define the initial state and goal state as initial_state = \"Arad\" goal_state = \"Bucharest\" Transition model and state space The transition model provides the way to identify the child of a node in a search tree given a specific action. In this problem, we need to translate the whole state space from the geographical network into the program together with the step costs between the connected states. The important information from the state space is the connections between states and the step costs between connected states. Notice that in this problem the connections are reversible. Therefore in our state space the connection from Arad to Zerind and the connection from Zerind to Arad are identical, hence only one instance of that connection is needed. The most straightforward way of defining the state space is by using a nested array, in which each inner array consists of the two connected states and its cost. Define a variable state_space to store the state space. The following code shows the first three elements in the nested array. Complete the definition of the variable by referring to the state space provided. state_space = [ [ 'Arad' , 'Zerind' , 75 ], [ 'Arad' , 'Timisoara' , 118 ], [ 'Timisoara' , 'Lugoj' , 111 ], ... ] In this problem, the actions are to travel to connected cities from the current city Nick is in. The transition model is directly defined by the action. Therefore, create a function to search through the state space to find the children of a node, which would suffice as actions and transition model. This function loops through the state_space variable to check for connections linked to the current node. The state on the connection becomes the child of the node in the search tree. We define a new function called expandAndReturnChildren . def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node : children . append ( n ) elif n == node : children . append ( m ) return children The function will provide a list of the children of the node . Node in the search tree We can use a class to represent each node in the search tree. The important information for each node includes the state and the parent of the node. class Node : def __init__ ( self , state = None , parent = None ): self . state = state self . parent = parent Class in Python In Python, to define a class, the class needs to have the function __init__ with at least one input self . This function is called when an object is initiated with this class. The internal variable self defines the properties of the object. The input arguments apart from self for the function __init__ are the parameters to be passed when initiating a new object. In a class, additional functions can also be defined, and these will be the methods for the object of this class. With this implementation of node as a class, the expandAndReturnChildren function is updated to be def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node . state : childnode = Node ( n , node ) children . append ( childnode ) elif n == node . state : childnode = Node ( m , node ) children . append ( childnode ) return children Quick check Your current code should look like initial_state = \"Arad\" goal_state = \"Bucharest\" state_space = [ [ 'Arad' , 'Zerind' , 75 ], [ 'Arad' , 'Timisoara' , 118 ], [ 'Timisoara' , 'Lugoj' , 111 ], ... ] class Node : def __init__ ( self , state = None , parent = None ): self . state = state self . parent = parent def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node . state : childnode = Node ( n , node ) children . append ( childnode ) elif n == node . state : childnode = Node ( m , node ) children . append ( childnode ) return children Breadth-first search algorithm Next we will create a function to execute the search algorithm. The first algorithm we will use is the breadth-first search (BFS). We will separate the problem definition from the algorithm definition, the algorithm function will have the inputs of the state space, initial state and goal state. def bfs ( state_space , initial_state , goal_state ): Separation of definitions for problem and algorithm The separation of problem definition and the algorithm definition allows us to re-use the algorithm easily for other problems and also re-use the problem definition with other algorithms In this function, we need two empty arrays to store the frontier and the explored states. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] In BFS, we will use the initial state as our root node. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] frontier . append ( Node ( initial_state , None )) Node(initial_state, None) As the root node has no parent, we use None as the value of the parent of root node. When we are generating the search tree, we will continually expanding the first node (FIFO) in the frontier until we generate a node with goal state. Therefore we need to have a loop to repeatedly expanding the first node in the frontier until the goal node is generated. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False frontier . append ( Node ( initial_state , None )) while not found_goal : ... In the loop we will first expand the first node in the frontier to obtain the children, and move the expanded node from frontier to the explored set. while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # copy the node to the explored set explored . append ( frontier [ 0 ]) # remove the expanded node from the frontier del frontier [ 0 ] Check if the children generated should be added to the frontier or discarded. If any child is expanded previously, i.e. in the explored set, or if it is generated previously but not expanded yet, i.e. in the frontier, then it should be discarded. Else, it should be added to the frontier. del frontier [ 0 ] # loop through the children for child in children : # check if a node was expanded or generated previously if not ( child . state in [ e . state for e in explored ]) and not ( child . state in [ f . state for f in frontier ]): # add child to the frontier frontier . append ( child ) List comprehension [e.state for e in explored] is using the syntax of list comprehension. List comprehension provides a shorter syntax to create a new list. This example loops through the variable explored and create a list with the values of .state for each element (node) in the explored list. This one-liner is equivalent to explored_nodes = [] for e in explored : explored_nodes . append ( e . state ) Before a child is added to the frontier, it should be tested for goal. If it has the goal state, the BFS algorithm should be terminated. if not ( child . state in [ e . state for e in explored ]) and not ( child . state in [ f . state for f in frontier ]): # goal test if child . state == goal_state : found_goal = True goal_node = child break # add child to the frontiers frontier . append ( child ) Now that the algorithm has identified the goal_node, we can trace the solution through the parent of the goal node all the way back to the root node (node with no parent). Then the function should return the solution of BFS. solution = [ goal_node . state ] trace_node = goal_node while trace_node . parent is not None : solution . insert ( 0 , trace_node . parent . state ) trace_node = trace_node . parent return solution Running the algorithm Now we have two functions expandAndReturnChildren and bfs , alongside with the variables state_space , initial_state , and goal_state . To run a script to execute the bfs function, we can have the script file structured as such: ```python initial_state = 'Arad' goal_state = 'Bucharest' state_space = [ ... ] class Node: ... def expandAndReturnChildren(...): ... def bfs(...): ... print('Solution: ' + str(bfs(state_space, initial_state, goal_state))) ``` Beware that in Python, a .py file can also be used to define a Python library/module. To prevent the commands outside the function to be executed when the file is used as a library instead of a script, we can implement an extra condition check. class Node : ... def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... if __name__ == '__main__' : state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' print ( 'Solution: ' + str ( bfs ( state_space , initial_state , goal_state ))) __name__ is a special variable in Python that evaluates to the name of the current module. This variable has the value of '__main__' if it's called as the main program rather than a module or library. This part is essentially the main function in other programming languages like C++ and Java. Execute the script and resolve any error. Question How can you modify the code to run other uninformed search algorithms such as uniform-cost search, depth-first search, etc.? Which part(s) of the code do you need to modify? You may extend the discussion to informed search. What would you modify the script to implement BFS on the vacuum world problem? Report Submit a report discussing the problems you have encountered, how you have solved them, and your answer for the questions.","title":"Lab 2: Breadth-First Search"},{"location":"archive/202301/lab2-uninformed-search/#lab-2-breadth-first-search","text":"","title":"Lab 2: Breadth-First Search"},{"location":"archive/202301/lab2-uninformed-search/#lab-learning-outcomes","text":"After completing this lab, the students are able to create Python script to execute breadth-first search algorithm to solve a search problem.","title":"Lab learning outcomes"},{"location":"archive/202301/lab2-uninformed-search/#nicks-route-finding-problem-in-romania","text":"The first search problem we are focusing on is Nick\u2019s route-finding problem in Romania, starting in Arad to reach Bucharest. The road map of Romania, which is the state space of the problem is given as follows: 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\", data: { circleradius: 10 } }); To solve the problem, we need to define the representation of the state, determine the transition model, and the execution of the search algorithm.","title":"Nick's route-finding problem in Romania"},{"location":"archive/202301/lab2-uninformed-search/#state-representation","text":"In this problem the only state we need to consider is the location of Nick. Therefore we can use the names of the cities as the state. Since Nick is starting in Arad, going to Bucharest, we can define the initial state and goal state as initial_state = \"Arad\" goal_state = \"Bucharest\"","title":"State representation"},{"location":"archive/202301/lab2-uninformed-search/#transition-model-and-state-space","text":"The transition model provides the way to identify the child of a node in a search tree given a specific action. In this problem, we need to translate the whole state space from the geographical network into the program together with the step costs between the connected states. The important information from the state space is the connections between states and the step costs between connected states. Notice that in this problem the connections are reversible. Therefore in our state space the connection from Arad to Zerind and the connection from Zerind to Arad are identical, hence only one instance of that connection is needed. The most straightforward way of defining the state space is by using a nested array, in which each inner array consists of the two connected states and its cost. Define a variable state_space to store the state space. The following code shows the first three elements in the nested array. Complete the definition of the variable by referring to the state space provided. state_space = [ [ 'Arad' , 'Zerind' , 75 ], [ 'Arad' , 'Timisoara' , 118 ], [ 'Timisoara' , 'Lugoj' , 111 ], ... ] In this problem, the actions are to travel to connected cities from the current city Nick is in. The transition model is directly defined by the action. Therefore, create a function to search through the state space to find the children of a node, which would suffice as actions and transition model. This function loops through the state_space variable to check for connections linked to the current node. The state on the connection becomes the child of the node in the search tree. We define a new function called expandAndReturnChildren . def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node : children . append ( n ) elif n == node : children . append ( m ) return children The function will provide a list of the children of the node .","title":"Transition model and state space"},{"location":"archive/202301/lab2-uninformed-search/#node-in-the-search-tree","text":"We can use a class to represent each node in the search tree. The important information for each node includes the state and the parent of the node. class Node : def __init__ ( self , state = None , parent = None ): self . state = state self . parent = parent Class in Python In Python, to define a class, the class needs to have the function __init__ with at least one input self . This function is called when an object is initiated with this class. The internal variable self defines the properties of the object. The input arguments apart from self for the function __init__ are the parameters to be passed when initiating a new object. In a class, additional functions can also be defined, and these will be the methods for the object of this class. With this implementation of node as a class, the expandAndReturnChildren function is updated to be def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node . state : childnode = Node ( n , node ) children . append ( childnode ) elif n == node . state : childnode = Node ( m , node ) children . append ( childnode ) return children Quick check Your current code should look like initial_state = \"Arad\" goal_state = \"Bucharest\" state_space = [ [ 'Arad' , 'Zerind' , 75 ], [ 'Arad' , 'Timisoara' , 118 ], [ 'Timisoara' , 'Lugoj' , 111 ], ... ] class Node : def __init__ ( self , state = None , parent = None ): self . state = state self . parent = parent def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node . state : childnode = Node ( n , node ) children . append ( childnode ) elif n == node . state : childnode = Node ( m , node ) children . append ( childnode ) return children","title":"Node in the search tree"},{"location":"archive/202301/lab2-uninformed-search/#breadth-first-search-algorithm","text":"Next we will create a function to execute the search algorithm. The first algorithm we will use is the breadth-first search (BFS). We will separate the problem definition from the algorithm definition, the algorithm function will have the inputs of the state space, initial state and goal state. def bfs ( state_space , initial_state , goal_state ): Separation of definitions for problem and algorithm The separation of problem definition and the algorithm definition allows us to re-use the algorithm easily for other problems and also re-use the problem definition with other algorithms In this function, we need two empty arrays to store the frontier and the explored states. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] In BFS, we will use the initial state as our root node. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] frontier . append ( Node ( initial_state , None )) Node(initial_state, None) As the root node has no parent, we use None as the value of the parent of root node. When we are generating the search tree, we will continually expanding the first node (FIFO) in the frontier until we generate a node with goal state. Therefore we need to have a loop to repeatedly expanding the first node in the frontier until the goal node is generated. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False frontier . append ( Node ( initial_state , None )) while not found_goal : ... In the loop we will first expand the first node in the frontier to obtain the children, and move the expanded node from frontier to the explored set. while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # copy the node to the explored set explored . append ( frontier [ 0 ]) # remove the expanded node from the frontier del frontier [ 0 ] Check if the children generated should be added to the frontier or discarded. If any child is expanded previously, i.e. in the explored set, or if it is generated previously but not expanded yet, i.e. in the frontier, then it should be discarded. Else, it should be added to the frontier. del frontier [ 0 ] # loop through the children for child in children : # check if a node was expanded or generated previously if not ( child . state in [ e . state for e in explored ]) and not ( child . state in [ f . state for f in frontier ]): # add child to the frontier frontier . append ( child ) List comprehension [e.state for e in explored] is using the syntax of list comprehension. List comprehension provides a shorter syntax to create a new list. This example loops through the variable explored and create a list with the values of .state for each element (node) in the explored list. This one-liner is equivalent to explored_nodes = [] for e in explored : explored_nodes . append ( e . state ) Before a child is added to the frontier, it should be tested for goal. If it has the goal state, the BFS algorithm should be terminated. if not ( child . state in [ e . state for e in explored ]) and not ( child . state in [ f . state for f in frontier ]): # goal test if child . state == goal_state : found_goal = True goal_node = child break # add child to the frontiers frontier . append ( child ) Now that the algorithm has identified the goal_node, we can trace the solution through the parent of the goal node all the way back to the root node (node with no parent). Then the function should return the solution of BFS. solution = [ goal_node . state ] trace_node = goal_node while trace_node . parent is not None : solution . insert ( 0 , trace_node . parent . state ) trace_node = trace_node . parent return solution","title":"Breadth-first search algorithm"},{"location":"archive/202301/lab2-uninformed-search/#running-the-algorithm","text":"Now we have two functions expandAndReturnChildren and bfs , alongside with the variables state_space , initial_state , and goal_state . To run a script to execute the bfs function, we can have the script file structured as such: ```python initial_state = 'Arad' goal_state = 'Bucharest' state_space = [ ... ] class Node: ... def expandAndReturnChildren(...): ... def bfs(...): ... print('Solution: ' + str(bfs(state_space, initial_state, goal_state))) ``` Beware that in Python, a .py file can also be used to define a Python library/module. To prevent the commands outside the function to be executed when the file is used as a library instead of a script, we can implement an extra condition check. class Node : ... def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... if __name__ == '__main__' : state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' print ( 'Solution: ' + str ( bfs ( state_space , initial_state , goal_state ))) __name__ is a special variable in Python that evaluates to the name of the current module. This variable has the value of '__main__' if it's called as the main program rather than a module or library. This part is essentially the main function in other programming languages like C++ and Java. Execute the script and resolve any error.","title":"Running the algorithm"},{"location":"archive/202301/lab2-uninformed-search/#question","text":"How can you modify the code to run other uninformed search algorithms such as uniform-cost search, depth-first search, etc.? Which part(s) of the code do you need to modify? You may extend the discussion to informed search. What would you modify the script to implement BFS on the vacuum world problem?","title":"Question"},{"location":"archive/202301/lab2-uninformed-search/#report","text":"Submit a report discussing the problems you have encountered, how you have solved them, and your answer for the questions.","title":"Report"},{"location":"archive/202301/lab3-informed-search/","text":"Lab 3: Informed Search Lab learning outcomes After completing this lab, the students are able to create Python script to execute an informed search algorithm to solve a search problem. Nick's route-finding problem in Romania The search problem we are focusing on for this lab is Nick\u2019s route-finding problem in Romania, starting in Arad to reach Bucharest. The road map of Romania, which is the state space of the problem is given as follows: 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\", data: { circleradius: 10 } }); Write a Python script to solve the problem with greedy-best-first search algorithm. In the process of writing the script, answer the following quesetions. Explain the logical flow of the transition model. What is a reasonable heuristic function for this problem? What is/are the difference(s) between breadth-first search algorithm and greedy-best-first search? How would these differences be reflected in the script? Explain the logical flow of sorting the frontier for node expansion. Explain the logical flow of identifying the solution when the goal node is found. Report Submit a written report that describes the considerations while writing the scripts, the answers to the above questions, and the problems you encountered in the process. Include your script in the written report as an appendix.","title":"Lab 3: Informed Search"},{"location":"archive/202301/lab3-informed-search/#lab-3-informed-search","text":"","title":"Lab 3: Informed Search"},{"location":"archive/202301/lab3-informed-search/#lab-learning-outcomes","text":"After completing this lab, the students are able to create Python script to execute an informed search algorithm to solve a search problem.","title":"Lab learning outcomes"},{"location":"archive/202301/lab3-informed-search/#nicks-route-finding-problem-in-romania","text":"The search problem we are focusing on for this lab is Nick\u2019s route-finding problem in Romania, starting in Arad to reach Bucharest. The road map of Romania, which is the state space of the problem is given as follows: 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\", data: { circleradius: 10 } }); Write a Python script to solve the problem with greedy-best-first search algorithm. In the process of writing the script, answer the following quesetions. Explain the logical flow of the transition model. What is a reasonable heuristic function for this problem? What is/are the difference(s) between breadth-first search algorithm and greedy-best-first search? How would these differences be reflected in the script? Explain the logical flow of sorting the frontier for node expansion. Explain the logical flow of identifying the solution when the goal node is found.","title":"Nick's route-finding problem in Romania"},{"location":"archive/202301/lab3-informed-search/#report","text":"Submit a written report that describes the considerations while writing the scripts, the answers to the above questions, and the problems you encountered in the process. Include your script in the written report as an appendix.","title":"Report"},{"location":"archive/202301/lab4-particle-swarm-optimisation/","text":"Lab 4: Particle Swarm Optimisastion Lab learning outcomes After completing this lab, the students are able to develop a Python function to perform global best particle swarm optimisation. Setup for Spyder If you are using Spyder for this lab, go to Tools > Preferences > IPython console > Graphics and set Backend to Automatic . Restart kernel by going to Consoles > Restart kernel . Problem to solve Solve the following problem using global best particle swarm optimisation: Problem Find the value of x to minimise the function \\(f(x) = (x+100)(x+50)(x)(x-20)(x-60)(x-100)\\) for \\(-100 < x < 100\\) Particle swarm optimisation particles initialisation personal best identification global best identification velocity calculation position update repeat from personal best identification until termination Parameter definition With global best particle swarm optimisaton, the position update function is given by \\[x_i(t+1) = x_i(t) + v_i(t+1)\\] and the velocity update function is \\[v_i(t+1) = v_i(t) + \\alpha_1\\beta_1(t) \\Big( p_i(t) - x_i(t) \\Big) + \\alpha_2\\beta_2(t)\\Big(p_g(t) - x_i(t)\\Big)\\] \u03b1 1 and \u03b1 2 are acceleration constants that are fixed throughout the algorithm. Define a small value for \u03b1 1 and \u03b1 2 , for example 0.1 . alpha = [ 0.1 , 0.1 ] \u03b2 1 (t) and \u03b2 2 (t) are random values between 0 and 1 that are regenerated every iteration. Therefore no definition is required. Also, define the number of particles to run the algorithm with. n_particle = 10 Place the definition of these variables in the __main__ block. if __name__ == '__main__' : alpha = [ 0.1 , 0.1 ] n_particle = 10 Create a class for particle As each particle is an individual, create a Particle class to hold the data of the particle's current position, velocity, and personal best position. class Particle : def __init__ ( self , position = 0 , velocity = 0 ): self . position = position self . velocity = velocity self . best_position = position Fitness function Fitness function is how we can compare different particles. As our goal is to minimise f(x) as stated in the beginning , we will use f(x) as our fitness function. By using f(x) in minimisation problem, it implies that the lower the value of f(x), the better the particle it is. The value of x is the position of the particle. Define the fitness function as a Python function. def fit_fcn ( position ): ... return fitness Initialise particles Particles are initialised with random positions within the constraints. At initialisation, we may assume that the initial velocities of all the particles. It is possible to initialise particles with non-zero velocities. For now, we will stick to zero initial velocities. Define a Python function that takes the input of the number of particles and the limits of the positions to initialise and return a list of objects of class Particle . Each particle has random position within the limits and zero velocity. def initialise_particles ( n_ptc , position_limits ): # position_limits is a list of two values. The first value is the lower boundary and the second value is the upper boundary. ... return particles Remember to test your function before proceed. Update personal best Create a method in the class Particle to update the best_position if necessary. class Particle : def __init__ ( ... ): ... def update_personal_best ( self ): # 1. calculate the fitnesses of the best_position and the particle's current position # 2. compare the fitnesses and determine if the current position is better than the best_position # 3. update if necessary # 4. no return statement is required If the new position has a lower fitness, i.e. the new position is better than the best position, update the best_position to hold the value of the new position. Update global best Initiate a variable named global_best_position with the value None in the __main__ block. Create a function that takes two positions as inputs, compare them, and return the better position of the two. def compareFitness ( pos1 , pos2 ): # 1. calculate the fitness of pos1 and pos2 # 2. compare to determine the better position return betterpos We will later use this function to compare the current global best position with the personal best position of each particle. Update velocity Create a method in the class Particle to update the velocity given \u03b1 1 , \u03b1 2 , \u03b2 1 , \u03b2 2 , and the global best position. class Particle : def __init__ ( ... ): ... def update_personal_best ( ... ): ... def update_velocity ( self , alpha , beta , glob_best_pos ): # alpha is a list of two values. we will access alpha_1 and alpha_2 by alpha[0] and alpha[1] respectively. This also applies to beta. # the current position, current velocity, and personal best position of the particle can be accessed by self.position, self.velocity, and self.best_position # assign the particle's velocity with the updated velocity Update particle position As updating a particle position only require information from within the particle object and the limits of the position, create a method called update_position in the class Particle taking the input of the limits of the position. class Particle : def __init__ ( ... ): ... def update_personal_best ( ... ): ... def update_velocity ( ... ): ... def update_position ( self , position_limits ): self . position = self . position + self . velocity # how should you solve the problem of the position (x) going out of the limits Create a loop (until termination) Consider the following termination criteria: exceeding 200 iterations fitnesses of all particles are close positions of all particles are close Create a function to calculate the average difference between the mean fitness and the fitness of each particle. def calc_avg_fit_diff ( particles ): # 1. calculate mean fitness of all particles # 2. calculate the difference between the mean fitness and the fitness of each particle # 3. calculate the average of the differences obtained from step 2 return avg_fit_diff Create a function to calculate the average difference between the mean position and the position of each particle. def calc_avg_pos_diff ( particles ): # 1. calculate mean position of all particles # 2. calculate the difference between the mean position and the position of each particle # 3. calculate the average of the differences obtained from step 2 return avg_pos_diff Create a loop (in the __main__ block) to execute the global best particle swarm optimisation (gbest PSO) until termination. if __name__ == '__main__' : # parameter initialisation alpha = [ 0.1 , 0.1 ] n_particle = 10 global_best_position = None position_limits = [ - 100 , 100 ] # termination threshold iteration = 0 max_iter = 200 min_avg_fit_diff = 0.1 min_avg_pos_diff = 0.1 # initialise particles particles = initialise_particles ( n_particle , position_limits ) while ( ... ): # how should you define the termination criteria here? print ( iteration , [ round ( x . position , 2 ) for x in particles ]) for particle in particles : # update personal best particle . update_personal_best () # update global best if global_best_position == None : global_best_position = particle . position else : global_best_position = compareFitness ( global_best_position , particle . position ) # generate beta randomly for current iteration beta = [ random . random (), random . random ()] for particle in particles : # update velocity particle . update_velocity ( alpha , beta , global_best_position ) # update position particle . update_position ( position_limits ) iteration += 1 # display results print ( iteration , [ round ( x . position , 2 ) for x in particles ]) Visualisation Let's add a few lines to visualise particles \"flying\" towards to optimal position. import the visualisation library import matplotlib.pyplot as plt add the following lines just before the while loop in the last code block in the previous section . This is to plot the existing particle positions on the graph. space_ax = plt . axes () space_ax . plot ( list ( range ( * position_limits )),[ fit_fcn ( x ) for x in range ( * position_limits )]) space_ax . set_title ( \"Position of particles in iteration {} \" . format ( iteration )) space_ax . set_xlabel ( \"Position\" ) space_ax . set_ylabel ( \"Fitness\" ) add the following lines between line 14 and line 15 in the last code block in the previous section , as well as after line 33. This is to remove the existing particle positions and plot the new positions, i.e to visually update the positions. if len ( space_ax . lines ) > 1 : del space_ax . lines [ 1 ] space_ax . plot ([ x . position for x in particles ], [ fit_fcn ( x . position ) for x in particles ], 'go' ) space_ax . set_title ( \"Position of particles in iteration {} \" . format ( iteration )) plt . pause ( 0.5 ) # pause the program for 0.5 second; if graph changes too quickly, increase this value; you can also speed up the process by decreasing this value Evaluation Store the values of the variables at each iteration for analysis and evaluation. position of each particle at each iteration (add the new line of code to the end of the methods) class Particle : def __init__ ( ... ): ... self . position_list = [ position ] def update_position ( ... ): ... self . position_list . append ( self . position ) velocity of each particle at each iteration (add the new line of code to the end of the methods) class Particle : def __init__ ( ... ): ... self . velocity_list = [ velocity ] def update_velocity ( ... ): ... self . velocity_list . append ( self . velocity ) personal best position of each particle at each iteration (add the new line of code to the end of the methods) class Particle : def __init__ ( ... ): ... self . best_position_list = [] def update_personal_best ( ... ): ... self . best_position_list . append ( self . best_position ) global best position at each iteration if __init__ == '__main__' : # parameter initialisation ... global_best_position_list = [] ... global_best_position = ... global_best_position_list . append ( global_best_position ) # take note on the indentation # generate beta randomly for current iteration ... Visualise the progression of these variables by adding the following code to the end of the __main__ block. [ pos_fig , position_axes ] = plt . subplots ( 4 , 1 , sharex = True ) position_axes [ 0 ] . set_title ( \"Position of each particle\" ) position_axes [ 1 ] . set_title ( \"Fitness of each particle\" ) position_axes [ 2 ] . set_title ( \"Boxplot of position at each iteration\" ) position_axes [ 3 ] . set_title ( \"Boxplot of fitness at each iteration\" ) position_axes [ 3 ] . set_xlabel ( \"Iteration\" ) [ vel_fig , velocity_axes ] = plt . subplots ( 2 , 1 , sharex = True ) velocity_axes [ 0 ] . set_title ( \"Velocity of each particle\" ) velocity_axes [ 1 ] . set_title ( \"Boxplot for velocity at each iteration\" ) velocity_axes [ 1 ] . set_xlabel ( \"Iteration\" ) [ p_best_fig , personal_best_axes ] = plt . subplots ( 4 , 1 , sharex = True ) personal_best_axes [ 0 ] . set_title ( \"Personal best position of each particle\" ) personal_best_axes [ 1 ] . set_title ( \"Personal best fitness of each particle\" ) personal_best_axes [ 2 ] . set_title ( \"Boxplot of personal best position at each iteration\" ) personal_best_axes [ 3 ] . set_title ( \"Boxplot of personal best fitness at each iteration\" ) personal_best_axes [ 3 ] . set_xlabel ( \"Iteration\" ) [ g_best_fig , global_best_axes ] = plt . subplots ( 2 , 1 , sharex = True ) global_best_axes [ 0 ] . set_title ( \"Global best position\" ) global_best_axes [ 1 ] . set_title ( \"Fitness for global best position\" ) global_best_axes [ 1 ] . set_xlabel ( \"Iteration\" ) for particle in particles : iteration_list = list ( range ( len ( particle . position_list ))) position_axes [ 0 ] . plot ( iteration_list , particle . position_list , '-o' ) position_axes [ 1 ] . plot ( iteration_list , [ fit_fcn ( x ) for x in particle . position_list ], '-o' ) velocity_axes [ 0 ] . plot ( iteration_list , particle . velocity_list , '-o' ) personal_best_axes [ 0 ] . plot ( iteration_list [: - 1 ], particle . best_position_list , '-o' ) personal_best_axes [ 1 ] . plot ( iteration_list [: - 1 ], [ fit_fcn ( x ) for x in particle . best_position_list ], '-o' ) position_axes [ 2 ] . boxplot ([[ p . position_list [ i ] for p in particles ] for i in iteration_list ], positions = iteration_list ) position_axes [ 3 ] . boxplot ([[ fit_fcn ( p . position_list [ i ]) for p in particles ] for i in iteration_list ], positions = iteration_list ) velocity_axes [ 1 ] . boxplot ([[ p . velocity_list [ i ] for p in particles ] for i in iteration_list ], positions = iteration_list ) personal_best_axes [ 2 ] . boxplot ([[ p . best_position_list [ i ] for p in particles ] for i in iteration_list [: - 1 ]], positions = iteration_list [: - 1 ]) personal_best_axes [ 3 ] . boxplot ([[ fit_fcn ( p . best_position_list [ i ]) for p in particles ] for i in iteration_list [: - 1 ]], positions = iteration_list [: - 1 ]) global_best_axes [ 0 ] . plot ( iteration_list [: - 1 ], global_best_position_list , '-o' ) global_best_axes [ 1 ] . plot ( iteration_list [: - 1 ], [ fit_fcn ( x ) for x in global_best_position_list ], '-o' ) Exercise Multiply the velocity memory, \\(v_i(t)\\) , with a value between 0 and 1, let's say 0.5. How does the process change? This is the effect of inertia weight. Reduce the value of \\(\\alpha_1\\) to 0.05 while maintaining \\(\\alpha_2\\) at 0.1 and investigate the effect. Reduce the value of \\(\\alpha_1\\) to 0. How does this affect the result? Modify such that \\(\\alpha_1\\) is larger than \\(\\alpha_2\\) . What's the effect? How may you modify the formulae for particles with two variables, in which the fitness function is defined as \\(f(x,y) = x^2 + y^2\\) ? Report Submit a report detailing the process you have gone through and the observations you have made.","title":"Lab 4: Particle Swarm Optimisastion"},{"location":"archive/202301/lab4-particle-swarm-optimisation/#lab-4-particle-swarm-optimisastion","text":"","title":"Lab 4: Particle Swarm Optimisastion"},{"location":"archive/202301/lab4-particle-swarm-optimisation/#lab-learning-outcomes","text":"After completing this lab, the students are able to develop a Python function to perform global best particle swarm optimisation.","title":"Lab learning outcomes"},{"location":"archive/202301/lab4-particle-swarm-optimisation/#setup-for-spyder","text":"If you are using Spyder for this lab, go to Tools > Preferences > IPython console > Graphics and set Backend to Automatic . Restart kernel by going to Consoles > Restart kernel .","title":"Setup for Spyder"},{"location":"archive/202301/lab4-particle-swarm-optimisation/#problem-to-solve","text":"Solve the following problem using global best particle swarm optimisation: Problem Find the value of x to minimise the function \\(f(x) = (x+100)(x+50)(x)(x-20)(x-60)(x-100)\\) for \\(-100 < x < 100\\)","title":"Problem to solve"},{"location":"archive/202301/lab4-particle-swarm-optimisation/#particle-swarm-optimisation","text":"particles initialisation personal best identification global best identification velocity calculation position update repeat from personal best identification until termination","title":"Particle swarm optimisation"},{"location":"archive/202301/lab4-particle-swarm-optimisation/#parameter-definition","text":"With global best particle swarm optimisaton, the position update function is given by \\[x_i(t+1) = x_i(t) + v_i(t+1)\\] and the velocity update function is \\[v_i(t+1) = v_i(t) + \\alpha_1\\beta_1(t) \\Big( p_i(t) - x_i(t) \\Big) + \\alpha_2\\beta_2(t)\\Big(p_g(t) - x_i(t)\\Big)\\] \u03b1 1 and \u03b1 2 are acceleration constants that are fixed throughout the algorithm. Define a small value for \u03b1 1 and \u03b1 2 , for example 0.1 . alpha = [ 0.1 , 0.1 ] \u03b2 1 (t) and \u03b2 2 (t) are random values between 0 and 1 that are regenerated every iteration. Therefore no definition is required. Also, define the number of particles to run the algorithm with. n_particle = 10 Place the definition of these variables in the __main__ block. if __name__ == '__main__' : alpha = [ 0.1 , 0.1 ] n_particle = 10","title":"Parameter definition"},{"location":"archive/202301/lab4-particle-swarm-optimisation/#create-a-class-for-particle","text":"As each particle is an individual, create a Particle class to hold the data of the particle's current position, velocity, and personal best position. class Particle : def __init__ ( self , position = 0 , velocity = 0 ): self . position = position self . velocity = velocity self . best_position = position","title":"Create a class for particle"},{"location":"archive/202301/lab4-particle-swarm-optimisation/#fitness-function","text":"Fitness function is how we can compare different particles. As our goal is to minimise f(x) as stated in the beginning , we will use f(x) as our fitness function. By using f(x) in minimisation problem, it implies that the lower the value of f(x), the better the particle it is. The value of x is the position of the particle. Define the fitness function as a Python function. def fit_fcn ( position ): ... return fitness","title":"Fitness function"},{"location":"archive/202301/lab4-particle-swarm-optimisation/#initialise-particles","text":"Particles are initialised with random positions within the constraints. At initialisation, we may assume that the initial velocities of all the particles. It is possible to initialise particles with non-zero velocities. For now, we will stick to zero initial velocities. Define a Python function that takes the input of the number of particles and the limits of the positions to initialise and return a list of objects of class Particle . Each particle has random position within the limits and zero velocity. def initialise_particles ( n_ptc , position_limits ): # position_limits is a list of two values. The first value is the lower boundary and the second value is the upper boundary. ... return particles Remember to test your function before proceed.","title":"Initialise particles"},{"location":"archive/202301/lab4-particle-swarm-optimisation/#update-personal-best","text":"Create a method in the class Particle to update the best_position if necessary. class Particle : def __init__ ( ... ): ... def update_personal_best ( self ): # 1. calculate the fitnesses of the best_position and the particle's current position # 2. compare the fitnesses and determine if the current position is better than the best_position # 3. update if necessary # 4. no return statement is required If the new position has a lower fitness, i.e. the new position is better than the best position, update the best_position to hold the value of the new position.","title":"Update personal best"},{"location":"archive/202301/lab4-particle-swarm-optimisation/#update-global-best","text":"Initiate a variable named global_best_position with the value None in the __main__ block. Create a function that takes two positions as inputs, compare them, and return the better position of the two. def compareFitness ( pos1 , pos2 ): # 1. calculate the fitness of pos1 and pos2 # 2. compare to determine the better position return betterpos We will later use this function to compare the current global best position with the personal best position of each particle.","title":"Update global best"},{"location":"archive/202301/lab4-particle-swarm-optimisation/#update-velocity","text":"Create a method in the class Particle to update the velocity given \u03b1 1 , \u03b1 2 , \u03b2 1 , \u03b2 2 , and the global best position. class Particle : def __init__ ( ... ): ... def update_personal_best ( ... ): ... def update_velocity ( self , alpha , beta , glob_best_pos ): # alpha is a list of two values. we will access alpha_1 and alpha_2 by alpha[0] and alpha[1] respectively. This also applies to beta. # the current position, current velocity, and personal best position of the particle can be accessed by self.position, self.velocity, and self.best_position # assign the particle's velocity with the updated velocity","title":"Update velocity"},{"location":"archive/202301/lab4-particle-swarm-optimisation/#update-particle-position","text":"As updating a particle position only require information from within the particle object and the limits of the position, create a method called update_position in the class Particle taking the input of the limits of the position. class Particle : def __init__ ( ... ): ... def update_personal_best ( ... ): ... def update_velocity ( ... ): ... def update_position ( self , position_limits ): self . position = self . position + self . velocity # how should you solve the problem of the position (x) going out of the limits","title":"Update particle position"},{"location":"archive/202301/lab4-particle-swarm-optimisation/#create-a-loop-until-termination","text":"Consider the following termination criteria: exceeding 200 iterations fitnesses of all particles are close positions of all particles are close Create a function to calculate the average difference between the mean fitness and the fitness of each particle. def calc_avg_fit_diff ( particles ): # 1. calculate mean fitness of all particles # 2. calculate the difference between the mean fitness and the fitness of each particle # 3. calculate the average of the differences obtained from step 2 return avg_fit_diff Create a function to calculate the average difference between the mean position and the position of each particle. def calc_avg_pos_diff ( particles ): # 1. calculate mean position of all particles # 2. calculate the difference between the mean position and the position of each particle # 3. calculate the average of the differences obtained from step 2 return avg_pos_diff Create a loop (in the __main__ block) to execute the global best particle swarm optimisation (gbest PSO) until termination. if __name__ == '__main__' : # parameter initialisation alpha = [ 0.1 , 0.1 ] n_particle = 10 global_best_position = None position_limits = [ - 100 , 100 ] # termination threshold iteration = 0 max_iter = 200 min_avg_fit_diff = 0.1 min_avg_pos_diff = 0.1 # initialise particles particles = initialise_particles ( n_particle , position_limits ) while ( ... ): # how should you define the termination criteria here? print ( iteration , [ round ( x . position , 2 ) for x in particles ]) for particle in particles : # update personal best particle . update_personal_best () # update global best if global_best_position == None : global_best_position = particle . position else : global_best_position = compareFitness ( global_best_position , particle . position ) # generate beta randomly for current iteration beta = [ random . random (), random . random ()] for particle in particles : # update velocity particle . update_velocity ( alpha , beta , global_best_position ) # update position particle . update_position ( position_limits ) iteration += 1 # display results print ( iteration , [ round ( x . position , 2 ) for x in particles ])","title":"Create a loop (until termination)"},{"location":"archive/202301/lab4-particle-swarm-optimisation/#visualisation","text":"Let's add a few lines to visualise particles \"flying\" towards to optimal position. import the visualisation library import matplotlib.pyplot as plt add the following lines just before the while loop in the last code block in the previous section . This is to plot the existing particle positions on the graph. space_ax = plt . axes () space_ax . plot ( list ( range ( * position_limits )),[ fit_fcn ( x ) for x in range ( * position_limits )]) space_ax . set_title ( \"Position of particles in iteration {} \" . format ( iteration )) space_ax . set_xlabel ( \"Position\" ) space_ax . set_ylabel ( \"Fitness\" ) add the following lines between line 14 and line 15 in the last code block in the previous section , as well as after line 33. This is to remove the existing particle positions and plot the new positions, i.e to visually update the positions. if len ( space_ax . lines ) > 1 : del space_ax . lines [ 1 ] space_ax . plot ([ x . position for x in particles ], [ fit_fcn ( x . position ) for x in particles ], 'go' ) space_ax . set_title ( \"Position of particles in iteration {} \" . format ( iteration )) plt . pause ( 0.5 ) # pause the program for 0.5 second; if graph changes too quickly, increase this value; you can also speed up the process by decreasing this value","title":"Visualisation"},{"location":"archive/202301/lab4-particle-swarm-optimisation/#evaluation","text":"Store the values of the variables at each iteration for analysis and evaluation. position of each particle at each iteration (add the new line of code to the end of the methods) class Particle : def __init__ ( ... ): ... self . position_list = [ position ] def update_position ( ... ): ... self . position_list . append ( self . position ) velocity of each particle at each iteration (add the new line of code to the end of the methods) class Particle : def __init__ ( ... ): ... self . velocity_list = [ velocity ] def update_velocity ( ... ): ... self . velocity_list . append ( self . velocity ) personal best position of each particle at each iteration (add the new line of code to the end of the methods) class Particle : def __init__ ( ... ): ... self . best_position_list = [] def update_personal_best ( ... ): ... self . best_position_list . append ( self . best_position ) global best position at each iteration if __init__ == '__main__' : # parameter initialisation ... global_best_position_list = [] ... global_best_position = ... global_best_position_list . append ( global_best_position ) # take note on the indentation # generate beta randomly for current iteration ... Visualise the progression of these variables by adding the following code to the end of the __main__ block. [ pos_fig , position_axes ] = plt . subplots ( 4 , 1 , sharex = True ) position_axes [ 0 ] . set_title ( \"Position of each particle\" ) position_axes [ 1 ] . set_title ( \"Fitness of each particle\" ) position_axes [ 2 ] . set_title ( \"Boxplot of position at each iteration\" ) position_axes [ 3 ] . set_title ( \"Boxplot of fitness at each iteration\" ) position_axes [ 3 ] . set_xlabel ( \"Iteration\" ) [ vel_fig , velocity_axes ] = plt . subplots ( 2 , 1 , sharex = True ) velocity_axes [ 0 ] . set_title ( \"Velocity of each particle\" ) velocity_axes [ 1 ] . set_title ( \"Boxplot for velocity at each iteration\" ) velocity_axes [ 1 ] . set_xlabel ( \"Iteration\" ) [ p_best_fig , personal_best_axes ] = plt . subplots ( 4 , 1 , sharex = True ) personal_best_axes [ 0 ] . set_title ( \"Personal best position of each particle\" ) personal_best_axes [ 1 ] . set_title ( \"Personal best fitness of each particle\" ) personal_best_axes [ 2 ] . set_title ( \"Boxplot of personal best position at each iteration\" ) personal_best_axes [ 3 ] . set_title ( \"Boxplot of personal best fitness at each iteration\" ) personal_best_axes [ 3 ] . set_xlabel ( \"Iteration\" ) [ g_best_fig , global_best_axes ] = plt . subplots ( 2 , 1 , sharex = True ) global_best_axes [ 0 ] . set_title ( \"Global best position\" ) global_best_axes [ 1 ] . set_title ( \"Fitness for global best position\" ) global_best_axes [ 1 ] . set_xlabel ( \"Iteration\" ) for particle in particles : iteration_list = list ( range ( len ( particle . position_list ))) position_axes [ 0 ] . plot ( iteration_list , particle . position_list , '-o' ) position_axes [ 1 ] . plot ( iteration_list , [ fit_fcn ( x ) for x in particle . position_list ], '-o' ) velocity_axes [ 0 ] . plot ( iteration_list , particle . velocity_list , '-o' ) personal_best_axes [ 0 ] . plot ( iteration_list [: - 1 ], particle . best_position_list , '-o' ) personal_best_axes [ 1 ] . plot ( iteration_list [: - 1 ], [ fit_fcn ( x ) for x in particle . best_position_list ], '-o' ) position_axes [ 2 ] . boxplot ([[ p . position_list [ i ] for p in particles ] for i in iteration_list ], positions = iteration_list ) position_axes [ 3 ] . boxplot ([[ fit_fcn ( p . position_list [ i ]) for p in particles ] for i in iteration_list ], positions = iteration_list ) velocity_axes [ 1 ] . boxplot ([[ p . velocity_list [ i ] for p in particles ] for i in iteration_list ], positions = iteration_list ) personal_best_axes [ 2 ] . boxplot ([[ p . best_position_list [ i ] for p in particles ] for i in iteration_list [: - 1 ]], positions = iteration_list [: - 1 ]) personal_best_axes [ 3 ] . boxplot ([[ fit_fcn ( p . best_position_list [ i ]) for p in particles ] for i in iteration_list [: - 1 ]], positions = iteration_list [: - 1 ]) global_best_axes [ 0 ] . plot ( iteration_list [: - 1 ], global_best_position_list , '-o' ) global_best_axes [ 1 ] . plot ( iteration_list [: - 1 ], [ fit_fcn ( x ) for x in global_best_position_list ], '-o' )","title":"Evaluation"},{"location":"archive/202301/lab4-particle-swarm-optimisation/#exercise","text":"Multiply the velocity memory, \\(v_i(t)\\) , with a value between 0 and 1, let's say 0.5. How does the process change? This is the effect of inertia weight. Reduce the value of \\(\\alpha_1\\) to 0.05 while maintaining \\(\\alpha_2\\) at 0.1 and investigate the effect. Reduce the value of \\(\\alpha_1\\) to 0. How does this affect the result? Modify such that \\(\\alpha_1\\) is larger than \\(\\alpha_2\\) . What's the effect? How may you modify the formulae for particles with two variables, in which the fitness function is defined as \\(f(x,y) = x^2 + y^2\\) ?","title":"Exercise"},{"location":"archive/202301/lab4-particle-swarm-optimisation/#report","text":"Submit a report detailing the process you have gone through and the observations you have made.","title":"Report"},{"location":"archive/202301/lab5-decision-tree/","text":"Lab 5: Decision Tree Lab learning outcomes After completing this lab, the students are able to construct decision tree using CART learning algorithm with the scikit-learn Python library. Datasets The iris dataset will be used for classification, and the diabetes dataset for regression. from sklearn import datasets import pandas as pd iris = datasets . load_iris () iris = { 'attributes' : pd . DataFrame ( iris . data , columns = iris . feature_names ), 'target' : pd . DataFrame ( iris . target , columns = [ 'species' ]), 'targetNames' : iris . target_names } diabetes = datasets . load_diabetes () diabetes = { 'attributes' : pd . DataFrame ( diabetes . data , columns = diabetes . feature_names ), 'target' : pd . DataFrame ( diabetes . target , columns = [ 'diseaseProgression' ]) } Split the datasets into 80-20 for train-test proportion. from sklearn.model_selection import train_test_split for dt in [ iris , diabetes ]: x_train , x_test , y_train , y_test = train_test_split ( dt [ 'attributes' ], dt [ 'target' ], test_size = 0.2 , random_state = 1 ) dt [ 'train' ] = { 'attributes' : x_train , 'target' : y_train } dt [ 'test' ] = { 'attributes' : x_test , 'target' : y_test } Note : Be reminded that random_state is used to reproduce the same \"random\" split of the data whenever the function is called. To produce randomly splitted data every time the function is called, remove the random_state argument. How do we access the training input data for the iris dataset? Decision tree Decision tree models and algorithms are provided by the scikit-learn as the class sklearn.tree.DecisionTreeClassifier for classification, and sklearn.tree.DecisionTreeRegressor for regression. Classification Import the class for decision tree classifier. from sklearn.tree import DecisionTreeClassifier Instantiate an object of DecisionTreeClassifier class with gini impurity as the split criterion. dtc = DecisionTreeClassifier ( criterion = 'gini' ) Train the classifier with the training data. We will use all the input attributes. dtc . fit ( iris [ 'train' ][ 'attributes' ], iris [ 'train' ][ 'target' ]) .predict function is used to predict the species of the testing data. predicts = dtc . predict ( iris [ 'test' ][ 'attributes' ]) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( iris [ 'test' ][ 'target' ] . species , predicts )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. accuracy = dtc . score ( iris [ 'test' ][ 'attributes' ], iris [ 'test' ][ 'target' ] . species ) print ( f 'Accuracy: { accuracy : .4f } ' ) Decision tree visualisation Import the matplotlib.pyplot library and the function to visualise the tree. import matplotlib.pyplot as plt from sklearn.tree import plot_tree Visualise the decision tree model. plt . figure ( figsize = [ 10 , 10 ]) tree = plot_tree ( dtc , feature_names = iris [ 'attributes' ] . columns . tolist (), class_names = iris [ 'targetNames' ], filled = True , rounded = True ) The maximum depth of a decision tree can be defined by adding the max_depth=... argument to the DecisionTreeClassifier(...) object instantiation. To allow unlimited maximum depth, pass max_depth=None . Create a loop to compare the accuracy of the prediction with different maximum depths. In every iteration, you should calculate both the accuracy on the training data and the accuracy on the testing data. The comparison should be shown in a graph with max_depth as the horizontal axis and accuracy as the vertical axis. Two lines should be displayed on the graph with one line for training accuracy and the other testing accuracy. Visualisation of decision surface This section explains the method to visualise a decision tree on a graph. To do so we will focus on using two input attributes, sepal length and sepal width, i.e. the first two columns. Instantiate the classifier without defining the maximum depth and train the model. dtc = DecisionTreeClassifier () input_cols = iris [ 'train' ][ 'attributes' ] . columns [: 2 ] . tolist () dtc . fit ( iris [ 'train' ][ 'attributes' ][ input_cols ], iris [ 'train' ][ 'target' ] . species ) Plot the decision tree. plt . figure ( figsize = [ 50 , 50 ]) plot_tree ( dtc , feature_names = input_cols , class_names = iris [ 'targetNames' ], filled = True , rounded = True ) plt . savefig ( 'classificationDecisionTreeWithNoMaxDepth.png' ) Prepare the colormaps. from matplotlib import cm from matplotlib.colors import ListedColormap colormap = cm . get_cmap ( 'tab20' ) cm_dark = ListedColormap ( colormap . colors [:: 2 ]) cm_light = ListedColormap ( colormap . colors [ 1 :: 2 ]) Calculating the decision surface. import numpy as np x_min = iris [ 'attributes' ][ input_cols [ 0 ]] . min () x_max = iris [ 'attributes' ][ input_cols [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = iris [ 'attributes' ][ input_cols [ 1 ]] . min () y_max = iris [ 'attributes' ][ input_cols [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = dtc . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision surface. plt . figure () plt . pcolormesh ( xx , yy , z , cmap = cm_light ) Plot the training and testing data. plt . scatter ( iris [ 'train' ][ 'attributes' ][ input_cols [ 0 ]], iris [ 'train' ][ 'attributes' ][ input_cols [ 1 ]], c = iris [ 'train' ][ 'target' ] . species , cmap = cm_dark , s = 200 , label = 'Training data' , edgecolor = 'black' , linewidth = 1 ) plt . scatter ( iris [ 'test' ][ 'attributes' ][ input_cols [ 0 ]], iris [ 'test' ][ 'attributes' ][ input_cols [ 1 ]], c = iris [ 'test' ][ 'target' ] . species , cmap = cm_dark , s = 200 , label = 'Testing data' , edgecolor = 'black' , linewidth = 1 , marker = '*' ) train_acc = dtc . score ( iris [ 'train' ][ 'attributes' ][ input_cols ], iris [ 'train' ][ 'target' ] . species ) test_acc = dtc . score ( iris [ 'test' ][ 'attributes' ][ input_cols ], iris [ 'test' ][ 'target' ] . species ) plt . title ( f 'training: { train_acc : .3f } , testing: { test_acc : .3f } ' ) plt . xlabel ( input_cols [ 0 ]) plt . ylabel ( input_cols [ 1 ]) plt . legend () You may not be able to see anything on one of the graph of the decision tree because the figure size is set to be larger than the screen size. However, the tree is saved to a png file in the same folder as your code. Overfitting Now, train a decision tree classifier of max_depth=3 with the two input attributes used in the previous section, sepal length and sepal width. Plot the decision surface for this classifier after the training. Compare the decision surface, training accuracy, and testing accuracy between this model and the model in the previous section. Discuss the comparison between the decision tree from previous section and that of max_depth=3 from the aspect of overfitting/generalisation. Regression Regression using decision tree can be achieved by using the DecisionTreeRegressor class in sklearn.tree . Instantiate a regressor class and train the regressor with the training data using all the input attributes. Predict the disease progression of the testing data, and determine the accuracy of the prediction. Create a plot of prediction accuracies against maximum depths of the decision tree for both training data and testing data. Visualisation of decision surface This section explains the method to visualise a decision tree on a graph. To do so we will focus on using two input attributes, age and bmi . Instantiate the classifier without defining the maximum depth and train the model. dtr = DecisiontTreeRegressor () input_cols = [ 'age' , 'bmi' ] dtr . fit ( diabetes [ 'train' ][ 'attributes' ][ input_cols ], diabetes [ 'train' ][ 'target' ] . diseaseProgression ) Plot the decision tree. plt . figure ( figsize = [ 50 , 50 ]) plot_tree ( dtr , feature_names = input_cols , filled = True , rounded = True ) plt . savefig ( 'regressionDecisionTreeWithNoMaxDepth.png' ) Prepare the colormaps. from matplotlib import cm dia_cm = cm . get_cmap ( 'Reds' ) Create the decision surface. import numpy as np x_min = diabetes [ 'attributes' ][ input_cols [ 0 ]] . min () x_max = diabetes [ 'attributes' ][ input_cols [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = diabetes [ 'attributes' ][ input_cols [ 1 ]] . min () y_max = diabetes [ 'attributes' ][ input_cols [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = dtr . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision surface plt . figure () plt . pcolormesh ( xx , yy , z , cmap = dia_cm ) Plot the training and testing data. plt . scatter ( diabetes [ 'train' ][ 'attributes' ][ input_cols [ 0 ]], diabetes [ 'train' ][ 'attributes' ][ input_cols [ 1 ]], c = diabetes [ 'train' ][ 'target' ] . diseaseProgression , label = 'Training data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( diabetes [ 'test' ][ 'attributes' ][ input_cols [ 0 ]], diabetes [ 'test' ][ 'attributes' ][ input_cols [ 1 ]], c = diabetes [ 'test' ][ 'target' ] . diseaseProgression , marker = '*' , label = 'Testing data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_cols [ 0 ]) plt . ylabel ( input_cols [ 1 ]) plt . legend () plt . colorbar () Overfitting Compare the decision tree regressor in the previous model with a decision tree regressor of a small maximum depth and discuss overfitting using the decision surface, training accuracy, and testing accuracy. Report Submit a written report that describes the considerations while writing the scripts, the answers to the above questions, and the problems you encountered in the process. Include your script in the written report as an appendix.","title":"Lab 5: Decision Tree"},{"location":"archive/202301/lab5-decision-tree/#lab-5-decision-tree","text":"","title":"Lab 5: Decision Tree"},{"location":"archive/202301/lab5-decision-tree/#lab-learning-outcomes","text":"After completing this lab, the students are able to construct decision tree using CART learning algorithm with the scikit-learn Python library.","title":"Lab learning outcomes"},{"location":"archive/202301/lab5-decision-tree/#datasets","text":"The iris dataset will be used for classification, and the diabetes dataset for regression. from sklearn import datasets import pandas as pd iris = datasets . load_iris () iris = { 'attributes' : pd . DataFrame ( iris . data , columns = iris . feature_names ), 'target' : pd . DataFrame ( iris . target , columns = [ 'species' ]), 'targetNames' : iris . target_names } diabetes = datasets . load_diabetes () diabetes = { 'attributes' : pd . DataFrame ( diabetes . data , columns = diabetes . feature_names ), 'target' : pd . DataFrame ( diabetes . target , columns = [ 'diseaseProgression' ]) } Split the datasets into 80-20 for train-test proportion. from sklearn.model_selection import train_test_split for dt in [ iris , diabetes ]: x_train , x_test , y_train , y_test = train_test_split ( dt [ 'attributes' ], dt [ 'target' ], test_size = 0.2 , random_state = 1 ) dt [ 'train' ] = { 'attributes' : x_train , 'target' : y_train } dt [ 'test' ] = { 'attributes' : x_test , 'target' : y_test } Note : Be reminded that random_state is used to reproduce the same \"random\" split of the data whenever the function is called. To produce randomly splitted data every time the function is called, remove the random_state argument. How do we access the training input data for the iris dataset?","title":"Datasets"},{"location":"archive/202301/lab5-decision-tree/#decision-tree","text":"Decision tree models and algorithms are provided by the scikit-learn as the class sklearn.tree.DecisionTreeClassifier for classification, and sklearn.tree.DecisionTreeRegressor for regression.","title":"Decision tree"},{"location":"archive/202301/lab5-decision-tree/#classification","text":"Import the class for decision tree classifier. from sklearn.tree import DecisionTreeClassifier Instantiate an object of DecisionTreeClassifier class with gini impurity as the split criterion. dtc = DecisionTreeClassifier ( criterion = 'gini' ) Train the classifier with the training data. We will use all the input attributes. dtc . fit ( iris [ 'train' ][ 'attributes' ], iris [ 'train' ][ 'target' ]) .predict function is used to predict the species of the testing data. predicts = dtc . predict ( iris [ 'test' ][ 'attributes' ]) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( iris [ 'test' ][ 'target' ] . species , predicts )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. accuracy = dtc . score ( iris [ 'test' ][ 'attributes' ], iris [ 'test' ][ 'target' ] . species ) print ( f 'Accuracy: { accuracy : .4f } ' ) Decision tree visualisation Import the matplotlib.pyplot library and the function to visualise the tree. import matplotlib.pyplot as plt from sklearn.tree import plot_tree Visualise the decision tree model. plt . figure ( figsize = [ 10 , 10 ]) tree = plot_tree ( dtc , feature_names = iris [ 'attributes' ] . columns . tolist (), class_names = iris [ 'targetNames' ], filled = True , rounded = True ) The maximum depth of a decision tree can be defined by adding the max_depth=... argument to the DecisionTreeClassifier(...) object instantiation. To allow unlimited maximum depth, pass max_depth=None . Create a loop to compare the accuracy of the prediction with different maximum depths. In every iteration, you should calculate both the accuracy on the training data and the accuracy on the testing data. The comparison should be shown in a graph with max_depth as the horizontal axis and accuracy as the vertical axis. Two lines should be displayed on the graph with one line for training accuracy and the other testing accuracy.","title":"Classification"},{"location":"archive/202301/lab5-decision-tree/#visualisation-of-decision-surface","text":"This section explains the method to visualise a decision tree on a graph. To do so we will focus on using two input attributes, sepal length and sepal width, i.e. the first two columns. Instantiate the classifier without defining the maximum depth and train the model. dtc = DecisionTreeClassifier () input_cols = iris [ 'train' ][ 'attributes' ] . columns [: 2 ] . tolist () dtc . fit ( iris [ 'train' ][ 'attributes' ][ input_cols ], iris [ 'train' ][ 'target' ] . species ) Plot the decision tree. plt . figure ( figsize = [ 50 , 50 ]) plot_tree ( dtc , feature_names = input_cols , class_names = iris [ 'targetNames' ], filled = True , rounded = True ) plt . savefig ( 'classificationDecisionTreeWithNoMaxDepth.png' ) Prepare the colormaps. from matplotlib import cm from matplotlib.colors import ListedColormap colormap = cm . get_cmap ( 'tab20' ) cm_dark = ListedColormap ( colormap . colors [:: 2 ]) cm_light = ListedColormap ( colormap . colors [ 1 :: 2 ]) Calculating the decision surface. import numpy as np x_min = iris [ 'attributes' ][ input_cols [ 0 ]] . min () x_max = iris [ 'attributes' ][ input_cols [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = iris [ 'attributes' ][ input_cols [ 1 ]] . min () y_max = iris [ 'attributes' ][ input_cols [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = dtc . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision surface. plt . figure () plt . pcolormesh ( xx , yy , z , cmap = cm_light ) Plot the training and testing data. plt . scatter ( iris [ 'train' ][ 'attributes' ][ input_cols [ 0 ]], iris [ 'train' ][ 'attributes' ][ input_cols [ 1 ]], c = iris [ 'train' ][ 'target' ] . species , cmap = cm_dark , s = 200 , label = 'Training data' , edgecolor = 'black' , linewidth = 1 ) plt . scatter ( iris [ 'test' ][ 'attributes' ][ input_cols [ 0 ]], iris [ 'test' ][ 'attributes' ][ input_cols [ 1 ]], c = iris [ 'test' ][ 'target' ] . species , cmap = cm_dark , s = 200 , label = 'Testing data' , edgecolor = 'black' , linewidth = 1 , marker = '*' ) train_acc = dtc . score ( iris [ 'train' ][ 'attributes' ][ input_cols ], iris [ 'train' ][ 'target' ] . species ) test_acc = dtc . score ( iris [ 'test' ][ 'attributes' ][ input_cols ], iris [ 'test' ][ 'target' ] . species ) plt . title ( f 'training: { train_acc : .3f } , testing: { test_acc : .3f } ' ) plt . xlabel ( input_cols [ 0 ]) plt . ylabel ( input_cols [ 1 ]) plt . legend () You may not be able to see anything on one of the graph of the decision tree because the figure size is set to be larger than the screen size. However, the tree is saved to a png file in the same folder as your code.","title":"Visualisation of decision surface"},{"location":"archive/202301/lab5-decision-tree/#overfitting","text":"Now, train a decision tree classifier of max_depth=3 with the two input attributes used in the previous section, sepal length and sepal width. Plot the decision surface for this classifier after the training. Compare the decision surface, training accuracy, and testing accuracy between this model and the model in the previous section. Discuss the comparison between the decision tree from previous section and that of max_depth=3 from the aspect of overfitting/generalisation.","title":"Overfitting"},{"location":"archive/202301/lab5-decision-tree/#regression","text":"Regression using decision tree can be achieved by using the DecisionTreeRegressor class in sklearn.tree . Instantiate a regressor class and train the regressor with the training data using all the input attributes. Predict the disease progression of the testing data, and determine the accuracy of the prediction. Create a plot of prediction accuracies against maximum depths of the decision tree for both training data and testing data.","title":"Regression"},{"location":"archive/202301/lab5-decision-tree/#visualisation-of-decision-surface_1","text":"This section explains the method to visualise a decision tree on a graph. To do so we will focus on using two input attributes, age and bmi . Instantiate the classifier without defining the maximum depth and train the model. dtr = DecisiontTreeRegressor () input_cols = [ 'age' , 'bmi' ] dtr . fit ( diabetes [ 'train' ][ 'attributes' ][ input_cols ], diabetes [ 'train' ][ 'target' ] . diseaseProgression ) Plot the decision tree. plt . figure ( figsize = [ 50 , 50 ]) plot_tree ( dtr , feature_names = input_cols , filled = True , rounded = True ) plt . savefig ( 'regressionDecisionTreeWithNoMaxDepth.png' ) Prepare the colormaps. from matplotlib import cm dia_cm = cm . get_cmap ( 'Reds' ) Create the decision surface. import numpy as np x_min = diabetes [ 'attributes' ][ input_cols [ 0 ]] . min () x_max = diabetes [ 'attributes' ][ input_cols [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = diabetes [ 'attributes' ][ input_cols [ 1 ]] . min () y_max = diabetes [ 'attributes' ][ input_cols [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = dtr . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision surface plt . figure () plt . pcolormesh ( xx , yy , z , cmap = dia_cm ) Plot the training and testing data. plt . scatter ( diabetes [ 'train' ][ 'attributes' ][ input_cols [ 0 ]], diabetes [ 'train' ][ 'attributes' ][ input_cols [ 1 ]], c = diabetes [ 'train' ][ 'target' ] . diseaseProgression , label = 'Training data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( diabetes [ 'test' ][ 'attributes' ][ input_cols [ 0 ]], diabetes [ 'test' ][ 'attributes' ][ input_cols [ 1 ]], c = diabetes [ 'test' ][ 'target' ] . diseaseProgression , marker = '*' , label = 'Testing data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_cols [ 0 ]) plt . ylabel ( input_cols [ 1 ]) plt . legend () plt . colorbar ()","title":"Visualisation of decision surface"},{"location":"archive/202301/lab5-decision-tree/#overfitting_1","text":"Compare the decision tree regressor in the previous model with a decision tree regressor of a small maximum depth and discuss overfitting using the decision surface, training accuracy, and testing accuracy.","title":"Overfitting"},{"location":"archive/202301/lab5-decision-tree/#report","text":"Submit a written report that describes the considerations while writing the scripts, the answers to the above questions, and the problems you encountered in the process. Include your script in the written report as an appendix.","title":"Report"},{"location":"archive/202301/pre-lab/","text":"Pre-lab: Basic Python Objective To understand basic syntax of Python programming language. Declare a variable Python is strongly and dynamically typed. Strongly typed means the type of a variable does not change unexpectedly. When a variable is defined as a string with only numerical digits, it stays string, it doesn\u2019t become an integer or number. Dynamically typed means the type of a variable can change when a value of a different type is assigned to the variable. As Python is dynamically typed, we do not need to specify a type when we declare a variable. We just assign a value to the variable. Define a variable named val and assign the value 'a' to the variable. val = 'a' The type of the variable can be viewed in the variable explorer. Continue from the previous code block, assign the value 1 to the same variable. The variable will now be of type int instead of type str . val = 1 Array manipulation Array in Python can be declared using a set of square brackets ( [...] ). To assign a variable with an empty array, arr = [] To define an array with a series of numbers, arr = [ 1 , 2 , 3 , 4 , 5 ] To define an array with a series of alphabets, arr = [ 'a' , 'b' , 'c' , 'd' , 'e' ] An array can also be defined with values of mixed types. arr = [ 1 , 'a' , 2 , 'b' , 3 , 'c' ] Python arrays (or more commonly known as lists) are zero indexed arrays; it means to access the first element in the array arr , # in IPython console arr [ 0 ] # gives the output of 1 arr [ 1 ] # gives the output of 'a' Python arrays also support negative indexing; this means to get the last element in the array arr , # in IPython console arr [ - 1 ] # gives the output of 'c' Colon ( : ) can be used to extract multiple elements from an array. Maximum two (2) colons can be used for indexing/slicing an array. arr[0:5:2] The value before the first colon is the starting index, the value after the first colon is the ending index (exclusive), the value after the second colon is the number of steps. If the first value is empty, it is assumed as 0 . If the second value is empty, it is assumed as the length of the array, i.e. up till the last element in the array. If the third value is empty, it is assumed as 1 . # in IPython console arr # [1, 'a', 2, 'b', 3, 'c'] arr [ 1 : 5 ] # ['a', 2, 'b', 3] arr [ 1 : 5 : 2 ] # ['a', 'b'] arr [: 3 ] # [1, 'a', 2] arr [ 4 :] # [3, 'c'] arr [ 2 : - 2 ] # [2, 'b'] arr [ 4 : 1 ] # [] arr [ 4 : 1 : - 1 ] # [3, 'b', 2] --> slice in the reverse order Add an element to the end of an array # in IPython console arr . append ( 4 ) arr # [1, 'a', 2, 'b', 3, 'c', 4] Add multiple elements to the end of an array # in IPython console arr . extend ([ 'd' , 5 , 'e' ]) arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Assign a value to a specific index # in IPython console arr [ 0 ] = 0 arr # [0, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Insert an element at a specific index # in IPython console arr . insert ( 1 , 1 ) arr # [0, 1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Remove an element at a specific index # in IPython console del arr [ 0 ] arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Combining arrays zip can be used to combine two or more arrays. # in editor joint_arr = list ( zip ([ 1 , 2 , 3 ], [ 'a' , 'b' , 'c' ])) # in IPython console joint_arr # [(1,'a'),(2,'b'),(3,'c')] Loops Python supports for and while loops. To loop through every element in the array arr and print them to the console, for a in arr : print ( a ) # 1 # a # 2 # ... for a in arr : loops through all elements in arr and in each loop, an element in arr is assigned to the variable a . Note that in most other programming languages, code blocks are separated with delimiters such as the curly brackets ( {} ). This is not the case in Python. Code blocks in Python are defined by their indentation and normally initiated with a colon( : ). For example, for a in arr : print ( a ) print ( arr ) print(a) is the command to be executed in each loop. print(arr) is only executed after the for loop is completed. for a in arr : print ( a ) print ( arr ) In this case, print(arr) is executed in each loop. Using zip we can loop through two arrays at once. for item in zip ([ 'a' , 'b' , 'c' , 'd' ],[ 'artificial' , 'breadth' , 'cost' , 'depth' ]): print ( item [ 0 ] + ' for ' + item [ 1 ]) enumerate is useful in obtaining the index of the element in the array. for ( index , item ) in enumerate ([ 'a' , 'b' , 'c' , 'd' ]): print ( 'Index of ' + item + ' is: ' + str ( index )) Looping through a dictionary ( dict ) can also be done easily. x_dict = { 'd' : 'depth' , 'e' : 'estimation' , 'f' : 'frontier' } for key , value in x_dict . items (): print ( key + ' for ' + value ) while loops can be defined similarly. x = 0 while x != 10 : x += 1 print ( x ) print ( 'while loop is completed' ) The following example introduces nested array and multiple assignments. arr = [[ 'a' , 1 ],[ 'b' , 2 ],[ 'c' , 3 ],[ 'd' , 4 ],[ 'e' , 5 ],[ 'f' , 6 ]] for [ a , n ] in arr : print ( str ( a ) + ' is ' + str ( n )) Conditions The following operators can be used for conditional testing: Operator Definition == Equivalence != Inequivalence < Less than <= Less than or equal to > Greater than >= Greater than or equal to Python also supports text operator for conditional testing: Operator Definition Example (symbolic) Example (text) is Equivalence a == 1 a is 1 not Inequivalence a != 1 a is not 1 or not (a is 1) or not a is 1 or not(a == 1) Combining two conditions can also be done with text operators and and or . Symbolic operator Text operator & and | or User-defined functions To define a custom function with the name of custom_fcn , def custom_fcn (): print ( 'This is a custom function to display custom message' ) To call the function, custom_fcn () # This is a custom function to display custom message User-defined functions with input arguments To define a custom function with input arguments, def custom_fcn ( msg ): print ( 'This is a custom function to display ' + msg ) The function can be called by custom_fcn ( 'new message' ) # This is a custom function to display new message User-defined functions with optional input arguments To define a custom function with optional input arguments, we just need to provide the default value to the optional input arguments. def custom_fcn ( msg = 'default message' ): print ( 'This is a custom function to display ' + msg ) The input arguments can also be specified as named inputs. custom_fcn ( msg = 'new message' ) # This is a custom function to display new message Exercise Create a new file in Spyder. Define a variable named friends such that it is a nested array in which contains the name, home country, and home state/province of 10 of your friends (real or virtual). For example, friends = [[ \"James\" , \"Malaysia\" , \"Malacca\" ], [ \"Goh\" , \"Australia\" , \"Brisbane\" ], [ \"Don\" , \"Malaysia\" , \"Pahang\" ]] Create a function in the same file with three (3) optional input arguments, name , home_country , home_state . def filterFriend ( name = \"\" , home_country = \"\" , home_state = \"\" ): ... return filtered This function will filter friends based on the input arguments provided. The function will ignore the input argument if it has empty string, i.e. \"\" . If any of the input arguments is provided, the function will find the friends whose detail(s) matches the input. For example, filterFriend(name=\"James\") will return [[\"James\", \"Malaysia\", \"Malacca\"]] filterFriend(home_country=\"Malaysia\") will return [[\"James\", \"Malaysia\", \"Malacca\"], [\"Don\", \"Malaysia\", \"Pahang\"]] .","title":"Pre-lab: Basic Python"},{"location":"archive/202301/pre-lab/#pre-lab-basic-python","text":"","title":"Pre-lab: Basic Python"},{"location":"archive/202301/pre-lab/#objective","text":"To understand basic syntax of Python programming language.","title":"Objective"},{"location":"archive/202301/pre-lab/#declare-a-variable","text":"Python is strongly and dynamically typed. Strongly typed means the type of a variable does not change unexpectedly. When a variable is defined as a string with only numerical digits, it stays string, it doesn\u2019t become an integer or number. Dynamically typed means the type of a variable can change when a value of a different type is assigned to the variable. As Python is dynamically typed, we do not need to specify a type when we declare a variable. We just assign a value to the variable. Define a variable named val and assign the value 'a' to the variable. val = 'a' The type of the variable can be viewed in the variable explorer. Continue from the previous code block, assign the value 1 to the same variable. The variable will now be of type int instead of type str . val = 1","title":"Declare a variable"},{"location":"archive/202301/pre-lab/#array-manipulation","text":"Array in Python can be declared using a set of square brackets ( [...] ). To assign a variable with an empty array, arr = [] To define an array with a series of numbers, arr = [ 1 , 2 , 3 , 4 , 5 ] To define an array with a series of alphabets, arr = [ 'a' , 'b' , 'c' , 'd' , 'e' ] An array can also be defined with values of mixed types. arr = [ 1 , 'a' , 2 , 'b' , 3 , 'c' ] Python arrays (or more commonly known as lists) are zero indexed arrays; it means to access the first element in the array arr , # in IPython console arr [ 0 ] # gives the output of 1 arr [ 1 ] # gives the output of 'a' Python arrays also support negative indexing; this means to get the last element in the array arr , # in IPython console arr [ - 1 ] # gives the output of 'c' Colon ( : ) can be used to extract multiple elements from an array. Maximum two (2) colons can be used for indexing/slicing an array. arr[0:5:2] The value before the first colon is the starting index, the value after the first colon is the ending index (exclusive), the value after the second colon is the number of steps. If the first value is empty, it is assumed as 0 . If the second value is empty, it is assumed as the length of the array, i.e. up till the last element in the array. If the third value is empty, it is assumed as 1 . # in IPython console arr # [1, 'a', 2, 'b', 3, 'c'] arr [ 1 : 5 ] # ['a', 2, 'b', 3] arr [ 1 : 5 : 2 ] # ['a', 'b'] arr [: 3 ] # [1, 'a', 2] arr [ 4 :] # [3, 'c'] arr [ 2 : - 2 ] # [2, 'b'] arr [ 4 : 1 ] # [] arr [ 4 : 1 : - 1 ] # [3, 'b', 2] --> slice in the reverse order","title":"Array manipulation"},{"location":"archive/202301/pre-lab/#add-an-element-to-the-end-of-an-array","text":"# in IPython console arr . append ( 4 ) arr # [1, 'a', 2, 'b', 3, 'c', 4]","title":"Add an element to the end of an array"},{"location":"archive/202301/pre-lab/#add-multiple-elements-to-the-end-of-an-array","text":"# in IPython console arr . extend ([ 'd' , 5 , 'e' ]) arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Add multiple elements to the end of an array"},{"location":"archive/202301/pre-lab/#assign-a-value-to-a-specific-index","text":"# in IPython console arr [ 0 ] = 0 arr # [0, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Assign a value to a specific index"},{"location":"archive/202301/pre-lab/#insert-an-element-at-a-specific-index","text":"# in IPython console arr . insert ( 1 , 1 ) arr # [0, 1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Insert an element at a specific index"},{"location":"archive/202301/pre-lab/#remove-an-element-at-a-specific-index","text":"# in IPython console del arr [ 0 ] arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Remove an element at a specific index"},{"location":"archive/202301/pre-lab/#combining-arrays","text":"zip can be used to combine two or more arrays. # in editor joint_arr = list ( zip ([ 1 , 2 , 3 ], [ 'a' , 'b' , 'c' ])) # in IPython console joint_arr # [(1,'a'),(2,'b'),(3,'c')]","title":"Combining arrays"},{"location":"archive/202301/pre-lab/#loops","text":"Python supports for and while loops. To loop through every element in the array arr and print them to the console, for a in arr : print ( a ) # 1 # a # 2 # ... for a in arr : loops through all elements in arr and in each loop, an element in arr is assigned to the variable a . Note that in most other programming languages, code blocks are separated with delimiters such as the curly brackets ( {} ). This is not the case in Python. Code blocks in Python are defined by their indentation and normally initiated with a colon( : ). For example, for a in arr : print ( a ) print ( arr ) print(a) is the command to be executed in each loop. print(arr) is only executed after the for loop is completed. for a in arr : print ( a ) print ( arr ) In this case, print(arr) is executed in each loop. Using zip we can loop through two arrays at once. for item in zip ([ 'a' , 'b' , 'c' , 'd' ],[ 'artificial' , 'breadth' , 'cost' , 'depth' ]): print ( item [ 0 ] + ' for ' + item [ 1 ]) enumerate is useful in obtaining the index of the element in the array. for ( index , item ) in enumerate ([ 'a' , 'b' , 'c' , 'd' ]): print ( 'Index of ' + item + ' is: ' + str ( index )) Looping through a dictionary ( dict ) can also be done easily. x_dict = { 'd' : 'depth' , 'e' : 'estimation' , 'f' : 'frontier' } for key , value in x_dict . items (): print ( key + ' for ' + value ) while loops can be defined similarly. x = 0 while x != 10 : x += 1 print ( x ) print ( 'while loop is completed' ) The following example introduces nested array and multiple assignments. arr = [[ 'a' , 1 ],[ 'b' , 2 ],[ 'c' , 3 ],[ 'd' , 4 ],[ 'e' , 5 ],[ 'f' , 6 ]] for [ a , n ] in arr : print ( str ( a ) + ' is ' + str ( n ))","title":"Loops"},{"location":"archive/202301/pre-lab/#conditions","text":"The following operators can be used for conditional testing: Operator Definition == Equivalence != Inequivalence < Less than <= Less than or equal to > Greater than >= Greater than or equal to Python also supports text operator for conditional testing: Operator Definition Example (symbolic) Example (text) is Equivalence a == 1 a is 1 not Inequivalence a != 1 a is not 1 or not (a is 1) or not a is 1 or not(a == 1) Combining two conditions can also be done with text operators and and or . Symbolic operator Text operator & and | or","title":"Conditions"},{"location":"archive/202301/pre-lab/#user-defined-functions","text":"To define a custom function with the name of custom_fcn , def custom_fcn (): print ( 'This is a custom function to display custom message' ) To call the function, custom_fcn () # This is a custom function to display custom message","title":"User-defined functions"},{"location":"archive/202301/pre-lab/#user-defined-functions-with-input-arguments","text":"To define a custom function with input arguments, def custom_fcn ( msg ): print ( 'This is a custom function to display ' + msg ) The function can be called by custom_fcn ( 'new message' ) # This is a custom function to display new message","title":"User-defined functions with input arguments"},{"location":"archive/202301/pre-lab/#user-defined-functions-with-optional-input-arguments","text":"To define a custom function with optional input arguments, we just need to provide the default value to the optional input arguments. def custom_fcn ( msg = 'default message' ): print ( 'This is a custom function to display ' + msg ) The input arguments can also be specified as named inputs. custom_fcn ( msg = 'new message' ) # This is a custom function to display new message","title":"User-defined functions with optional input arguments"},{"location":"archive/202301/pre-lab/#exercise","text":"Create a new file in Spyder. Define a variable named friends such that it is a nested array in which contains the name, home country, and home state/province of 10 of your friends (real or virtual). For example, friends = [[ \"James\" , \"Malaysia\" , \"Malacca\" ], [ \"Goh\" , \"Australia\" , \"Brisbane\" ], [ \"Don\" , \"Malaysia\" , \"Pahang\" ]] Create a function in the same file with three (3) optional input arguments, name , home_country , home_state . def filterFriend ( name = \"\" , home_country = \"\" , home_state = \"\" ): ... return filtered This function will filter friends based on the input arguments provided. The function will ignore the input argument if it has empty string, i.e. \"\" . If any of the input arguments is provided, the function will find the friends whose detail(s) matches the input. For example, filterFriend(name=\"James\") will return [[\"James\", \"Malaysia\", \"Malacca\"]] filterFriend(home_country=\"Malaysia\") will return [[\"James\", \"Malaysia\", \"Malacca\"], [\"Don\", \"Malaysia\", \"Pahang\"]] .","title":"Exercise"}]}